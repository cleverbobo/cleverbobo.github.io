<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>生成式对抗神经网络(待更新)</title>
    <url>/2020/10/09/GAN/</url>
    <content><![CDATA[<p>最近不想更新，暂时留个坑，以后更新~</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>解析论文《Context-aware Human Motion Prediction》</title>
    <url>/2020/09/03/HMP/</url>
    <content><![CDATA[<p>论文原文地址：<a href="https://cleverbobo.github.io/file/hmp.pdf">https://cleverbobo.github.io/file/hmp.pdf</a><br>仅供学术交流使用~</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><blockquote>
<blockquote>
<p>The problem of predicting human motion given a sequence of past observations is at the core of many applications in robotics and computer vision. Current state-of-the-art formulate this problem as a sequence-to-sequence task, in which a historical of 3D skeletons feeds a Recurrent Neural Network (RNN) that predicts future movements, typically in the order of 1 to 2 seconds. However , one aspect that has been obviated so far , is the fact that human motion is inherently driven by interactions with objects and/or other humans in the environment.<br>In this paper , we explore this scenario using a novel context-aware motion prediction architecture. We use a semantic-graph model where the nodes parameterize the human and objects in the scene and the edges their mutual interactions. These interactions are iteratively learned through a graph attention layer , fed with the past observations, which now include both object and human body motions. Once this semantic graph is learned, we inject it to a standard RNN to predict future movements of the human/sand object/s. We consider two variants of our architecture, either freezing the contextual interactions in the future of updating them. A thorough evaluation in the “Whole-Body Human Motion Database” [29] shows that in both cases,our context-aware networks clearly outperform baselines in which the context information is not considered.</p>
</blockquote>
</blockquote>
<p><strong>翻译：</strong><br>&emsp;&emsp;在智能机器人和计算机视觉的许多应用中，根据过去的一系列对人类动作的观察结果来预测人体运动是一个核心问题。目前，主要应用Seq2Seq技术来处理相关的领域，其中采用3D骨骼(人行为动作的抽象表达方式）作为历史输入，为循环神经网络(RNN)提供信息从而预测未来的运动。这个未来的时间段通常控制在1到2秒的时间内。然而，到目前为止，有一个观点已被人们否定——人类的运动<strong>从本质上</strong>来说，是由与环境中的物体或其他人的互动所<strong>驱动</strong>的。<br><a id="more"></a><br>&emsp;&emsp;在本文中，我们使用一种<strong>新的上下文感知</strong>的运动预测架构来预测人类行为。我们使用一种语义图模型，图中的节点代表场景中参数化后的人或物，以及它们之间的相互作用(具体表现为权重）。这些权重的大小是通过在图表注意力层不断迭代学习的，影响迭代的因素有过去的观察，现在包括物体和人体运动。一旦这个语义图模型被训练成功，我们将它与一个标准的RNN模型相结合，来预测人/物的未来运动。目前，我们主要考虑架构的两种变体，即是否在将来更新它们的权重时，保留上下文交互。在“全身人体运动数据库”[29]中进行的全面评估表明，在这两种情况下，我们的上下文感知网络性能明显优于没有考虑上下文信息。</p>
<p>关键字解析：<em>人类行为预测</em>，<em>上下文感知</em>，<em>注意力机制</em>，<em>RNN</em></p>
<h2 id="实验数据解析"><a href="#实验数据解析" class="headerlink" title="实验数据解析"></a>实验数据解析</h2><p><img src="/2020/09/03/HMP/1.PNG" alt><br>预测目标：human motion（人类行为）、object motion(物体行为)<br>模型：ZV，RNN，QuarterNet,C-RNN,C-RNN+OMP，C-RNN+LI,C-RNN+OMP+LI<br>行为种类：Passing objects(路过)、 Grasping objects(抓取)、 Cutting food(切菜) 、Mixing objects(搅拌)、 Cooking(做饭)<br>ps:中间的数字大小代表误差，用黑体表注的为误差(L2范数)最小的情况</p>
<p><img src="/2020/09/03/HMP/2.PNG" alt><br>&emsp;&emsp;具体的运动生成时间长达两秒钟。最左:使用的预测模型以及基线的预测样本框架；中间:地面人的实际行为的框架图与模型得出的预测框架图，其中，人和物体的运动分别从浅蓝色到深蓝色和从浅绿色到深绿色来表示。从上到下的动作依次是一个人支撑在桌子上踢箱子，一个人靠在桌子上，两个人(其中一个站在梯子上)经过一个物体。最右:预测的邻接矩阵<strong>(上侧代表影响因素，左侧代表受影响因素)</strong>，它代表了我们的模型目标之间的交互。注意，这些关系是有方向的(例如，在最后一个例子中，梯子高度影响人类#1的运动(50%)，但是人类对梯子几乎没有影响(11%)。最佳放大彩色视图。</p>
<p><img src="/2020/09/03/HMP/3.PNG" alt><br>&emsp;&emsp;在过去对上下文的观察中，模型细化的平均交互。在左边和中间的图中，我们分别描述了桌子清洁和移动盒子活动的相关交互。在第一种情况下，请注意桌子对海绵和人的影响很大，因为海绵和人最初会移向桌子进行清洁。类似地，在第二种情况下，人类走向地上的一个盒子，把它捡起来，放在桌子上。右图显示了相关对象类型的所有测试样本的平均自交互百分比。我们发现，像桌子或梯子这样的非移动物体总是很少受到其他物体的影响。同样，被动的物体，如刀子或瓶子，经常被人移动，更容易受到它们的影响，自我影响相对较低。</p>
<p><img src="/2020/09/03/HMP/4.PNG" alt><br>&emsp;&emsp;人体和物体运动预测中对噪声的鲁棒性。使用原始测试集(无噪声输入)时主要模型的平均性能，与看到有噪声的观测值时的性能相比。</p>
<p><img src="/2020/09/03/HMP/5.PNG" alt><br>&emsp;&emsp;CMU数据集中预测误差的平均值和标准差。</p>
<h2 id="实验结论"><a href="#实验结论" class="headerlink" title="实验结论"></a>实验结论</h2><blockquote>
<blockquote>
<p>In this work, we explore a context-aware motion prediction architecture, using a semantic-graph representation where objects and humans are represented by nodes independently of the number of objects or complexity of the environment. We extensively analyze their contribution for human motion prediction. The results observed in different actions suggest that the models proposed are able to understand human activities significantly better than state-of-art models which do not use context, improving both human and object motion prediction.</p>
</blockquote>
</blockquote>
<p><strong>翻译：</strong><br>在本中，我们提出了一种上下文感知的运动预测架构，使用语义图表示。其中，对象和人由节点表示，与对象的数量或环境的复杂性无关。我们广泛分析了它们对人类运动预测的贡献。在不同动作中观察到的结果表明，所提出的模型能够比不使用上下文的最先进的模型更好地理解人类活动，从而改进了人类和物体的运动预测。</p>
<h2 id="论文具体内容"><a href="#论文具体内容" class="headerlink" title="论文具体内容"></a>论文具体内容</h2><p><img src="/2020/09/03/HMP/7.PNG" alt></p>
<h3 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h3><p>该篇论文主要参考了<a href="https://cleverbobo.github.io/file/hm.pdf">《On human motion prediction using recurrent neural networks》</a>提出的架构，并在此基础上提出了改进，所以先简单介绍下这篇论文介绍了什么。作者比较了其他一些网络结构的人体运动预测结果，发现存在各种各样的问题，比如第一帧不连续，调优困难，模型复杂，只能针对特定动作等等，并提出了以下架构：<br><img src="/2020/09/03/HMP/6.PNG" alt><br>这就是一个典型的sequence2sequence问题，处理序列问题一般都是使用RNN网络，并提出Sampling-based loss 解决超参数调优问题，Residual architecture解决第一帧不连续问题。</p>
<ul>
<li><p>sequence2sequence<br>由encoder和decoder基本结构组成，把输入序列转换为向量，此为编码，然后输出端把预测向量再转化为输出序列，此为解码。</p>
</li>
<li><p>sampling based loss<br>序列问题需要关心时间上的相关性，所以采用RNN结构。但是RNN结构如果只是单纯学习真实值的话，会无法从自己的错误中恢复，如果加入噪音会使得模型难以调参。作者提出了的方法，在训练的过程中，把decoder的输出作为自己的输入。</p>
</li>
<li><p>residual architecture<br>为了解决第一帧预测值不连续的问题，作者提出了residual architecture。原理为添加了速度这一概念，并替换之前认为的预测人体的姿势。即每一帧的预测相当于预测速度的变化值，而不是预测人体姿势本身。所以第一帧的预测便简化为0速度或者接近0速度的预测。而实现方式也较为简单，在每一个RNN模块的输入和输出上添加一个链接，从而学习速度的变化。</p>
</li>
</ul>
<p><strong>实验结论：</strong></p>
<ol>
<li>作者基于sequence-to-sequence模型之上进行了一系列优化用于解决使用RNN所碰到的问题。</li>
<li>使用Sampling-based loss，在训练阶段将decoder预测的值加入自己的输入中以解决RNN只学习真实值而不能从自己错误中恢复的问题。</li>
<li>使用residual connection，以学习速度代替学习人物姿势本身做到消除第一帧不连续的问题，通过在每个RNN模块建立一条输入和输出的链接实现。</li>
</ol>
<h3 id="改进部分：背景感知"><a href="#改进部分：背景感知" class="headerlink" title="改进部分：背景感知"></a>改进部分：背景感知</h3><p>由于当前使用的主流数据集（仅标注人类运动，不标注背景物块的运动）和某些论证（背景物块的运动不会在本质上影响人类运动）的影响，导致在人类行为预测的问题上下意识忽略背景（context）信息，而这篇论文采用不同的数据集，在原有的模型基础上考虑背景信息，用于提升预测的准确性。</p>
<ul>
<li><p>Learning interactions<br>各个物体之间相互作用关系用一个邻接矩阵$A$表示，刚开始将邻接矩阵$A$初始化为对角阵，也就是各个目标之间相互独立，然后根据每个节点的隐藏状态$H$计算相互作用的大小：</p>
<script type="math/tex; mode=display">A_{i j}^{t}=g\left(H_{i}^{t}, H_{i}^{t}-H_{j}^{t}\right)</script><p>并以非监督学习的方式更新不同目标之间的邻接矩阵</p>
</li>
<li><p>Object motion prediction<br>该文作者除了预测人体运动之外，还同时预测了背景（context）的运动信息。该方法使用过去的背景信息以及每个对象隐藏状态上的residual architecture来预测所有对象的对象运动，预测的位置会被作为下一步的输入。</p>
</li>
</ul>
<h3 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h3><ul>
<li>相比于其他的模型，改进(“OMP”和“LI”为后缀的模型)的模型能在大多数测试中提供最好的结果，具体数据请参考前面的数据分析。</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>课程</tag>
        <tag>进阶</tag>
        <tag>英文论文解析</tag>
      </tags>
  </entry>
  <entry>
    <title>变分自编码器(待更新)</title>
    <url>/2020/10/09/VAEs/</url>
    <content><![CDATA[<p>暂时不想更新，先留个坑，不定期会补上~</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>通俗理解Seq2Seq模型+Attention机制(待修正)</title>
    <url>/2020/09/06/attention/</url>
    <content><![CDATA[<h2 id="seq2seq模型"><a href="#seq2seq模型" class="headerlink" title="seq2seq模型"></a>seq2seq模型</h2><p>seq2seq本质上是一种encoder-decoder的框架，最典型的应用就是机器翻译问题，模型先使用编码器对源语言进行编码，得到固定长度的编码向量，然后在对该编码向量进行解码，得到对应的翻译语言向量。因为输入和输出都是序列数据，因此又被称为sequence-to-sequence，简称seq2seq，目前应用最多的编码解码器是RNN(LSTM，GRU)，但编码解码器并不仅限于RNN。<br><a id="more"></a></p>
<h3 id="传统的几种seq2seq方式"><a href="#传统的几种seq2seq方式" class="headerlink" title="传统的几种seq2seq方式"></a>传统的几种seq2seq方式</h3><p>1.输入 $x$通过encoder编码，即RNN结构(这里以RNN结构为例，但不仅限于RNN结构)通过隐藏状态$h_{1}, h_{2}, \ldots, h_{t}$的不断传播，在传播过程中同时会产生输出 $y_{1}, y_{2}, \ldots, y_{t}$ ，然后形成编码向量 $\left[y_{1}, y_{2}, \ldots, y_{t}\right]$ ，在解码过程中编码向量每一步都会输入给decoder。<br><img src="/2020/09/06/attention/1.jpg" alt><br>2.如果decoder同样也是个RNN结构的话，那么可以利用上一时刻的输出来帮助预测。encoder的编码赋值给了decoder的初始hidden。decoder的初始标签是<SOS>特色的开始标签，对应着词表中这个标签的embedding。<br><img src="/2020/09/06/attention/2.jpg" alt><br>3.结合了前两种模式，decoder每一时刻的输入不仅有来自encoder的编码向量，还有上一时刻的输出，同时还有上一时刻的hidden。<br><img src="/2020/09/06/attention/3.jpg" alt><br>4.带有attention机制的编解码，encoder的RNN每一步都有一个输出 [公式] ，给每一个 [公式] 一个权重，计算带权求和的向量。这样做的好处是，每预测一个词都和原文本的部分最相关。<br><img src="/2020/09/06/attention/4.jpg" alt></SOS></p>
<h3 id="attention编解码的过程"><a href="#attention编解码的过程" class="headerlink" title="attention编解码的过程"></a>attention编解码的过程</h3><p>输人： $\quad x=\left(x_{1}, \ldots, x_{T_{x}}\right),$ 输出： $\quad y=\left(y_{1}, \ldots, y_{T_{y}}\right)$<br>$h_{t}=R N N_{\text {encoder }}\left(x_{t}, h_{t-1}\right),$ Encoder方面接受每个单词的embedding以及上一个时 间点的hidden state, 输出是该时间点的hidden state。 $s_{t}=R N N_{\text {decoder}}\left(y_{t-1}^{\tilde{\tau}}, s_{t-1}\right),$ Decoder方面接受的是目标句子里单词的word<br>embedding以及上一个时间点的hidden state。在计算 $s_{1}$ 时, $\quad \tilde{y}_{0}$ 即encoder的编码向量, 可以是Encoder最后的hidden state, $\quad h_{t}$ 通过decoder的hidden states和encoder的hidden states来计算一个分数 $e_{i j}=\operatorname{score}\left(s_{i}, h_{j}\right)$<br>it算每一个encoder的hidden states的权重: $\quad \alpha_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{T_{x}} \exp \left(e_{i k}\right)}$<br>・ 最后得到context vector，是对于encoder输出的hidden states的一个加权平均 $c_{i}=\sum_{j=1}^{T_{x}} \alpha_{i j} h_{j}$</p>
<ul>
<li>将context vector和decoder的hidden states串接起来 $\tilde{s}_{t}=\tanh \left(W_{c}\left[c_{t} ; s_{t}\right]\right)$<br>・计算最后的输出概率： $\quad p\left(y_{t} \mid y&lt;t, x\right)=s o f t m a x\left(W_{s} \tilde{s}_{t}\right)$<br>关于上式中的score方法：<br>(1) dot score function: 输入是encoder的所有hidden states H: 大小为(hid dim,seq length)。decoder在一个时间点上的hidden state, s: 大小为(hid dim, 1)。<br>・ 旋转H为(seq length, hid dim)与s做点杰得到大小为(sequence length,<br>1)的分数。<br>・ 对分数做softmax得到合为1的权重。 将H与上一步得到的权重做点乘得到大小为(hid dim, 1)的context vector。<br>(2) General score function: 输入是encoder的所有hidden states H: 大小为(hid $\operatorname{dim}_{-} 1,$ seq length), decoder在一个时间点上的hidden state, s: 大小为(hid dim_2, 1), 此处两个hidden state的纬度不一致。<br>旋转H为(seq length, hid dim1)与Wa(大小为hid dim_1,hid dim_2)做点乘，再和s做点乘得 到一个大小为(seq length, 1)的分数。<br>・ 对分数做softmax得到一个合为1的权重。 将H与第二步得到的权重做点拜得到一个大小为(hid dim_1, 1)的context vector。</li>
</ul>
<h3 id="attention机制的原理"><a href="#attention机制的原理" class="headerlink" title="attention机制的原理"></a>attention机制的原理</h3><p>神经学中的注意力机制有两种：</p>
<p>(1)自上而下的有意识的注意力，称为聚焦式注意力(Focus Attention)，聚焦式注意力是指有预定目的，依赖任务的，主动有意识地聚焦与某一对象的注意力。</p>
<p>(2)自下而上的无意识的注意力，称为基于显著性注意力(Saliency-Based Attention)。是由外界刺激驱动的注意，不需要主动去干预，和任务无关。如果一个对象的刺激信息不同于其周围信息，一种无意识的“赢者通吃”(Winner-Take-All)或者门控(Gating)机制就可以把注意力转向这个对象。</p>
<p>当你在听你的朋友说话时，你专注于你朋友的说话而忽略了周围其他的声音，这就是聚焦式注意力，当你在其他周围声音中听到了感兴趣的词时，你会马上注意到，这是显著性注意力。</p>
<p>给定N组信息输入 $X=\left[x_{1}, x_{2}, \ldots, x_{N}\right],$ 每个向量 $x_{i}, i \in[1, N]$ 都表示一组输入信 息。注意力机馳的计算分为两步：一是在所有输入信息上计算注意力分布, 二是根据注意力分布来 计算输入信息的加权平均。</p>
<p>注意力分布：为了从N个输入向量 $\left[x_{1}, x_{2}, \ldots, x_{N}\right]$ 中选择出和某个特定任务相关的信息, 需 要引入一个和任务相关的表示称为直询向量(Query Vector)，并通过一个打分函数score0计算每 个输入向量和直词向量之间的相关性。<br>给定一个和任务相关的直词向量q, 用注意力变量 $z \in[1, N]$ 来表示被选择信息的索引位置, 即 $z=i$ 表示选择了第i个输入向量。首先计算在给定q和X下，选择第i个输入向量的概率 $\alpha_{i}$ (权 重)</p>
<script type="math/tex; mode=display">
\alpha_{i}=p(z=i \mid X, q)=\operatorname{softmax}\left(s\left(x_{i}, q\right)\right)=\frac{\exp \left(s\left(x_{i}, q\right)\right)}{\sum_{j=1}^{N} \exp \left(s\left(x_{j}, q\right)\right)}</script><p>$\alpha_{i}$ 为注意力分布权重, $\quad s\left(x_{i}, q\right)$ 为注意力打分函数, 打分函数有很多种方式比如 :</p>
<ol>
<li>加性模型: $\quad s\left(x_{i}, q\right)=v^{T} \tanh \left(W x_{i}+U q\right)$</li>
<li>点积模型： $\quad s\left(x_{i}, q\right)=x_{i}^{T} q$</li>
<li>缩放点积模型: $\quad s\left(x_{i}, q\right)=\frac{x_{i}^{T} q}{\sqrt{d}}$</li>
<li>双线性模型: $\quad s\left(x_{i}, q\right)=x_{i}^{T} W q$<br>$W, U, v$ 为可学习的参数, $\quad d$ 为输入向量的维度。<br>点积模型在实现上更好的利用了矩阵乘积, 从而计算效率更高, 但是当输入向量的维度d比较高, 点积模型的值通常有比较大的方差, 从而导致softmax函数的梯度比较小，缩放点积模型可以较好 的解决此问题。双线性模型可以看作是一种泛化的点积模型, 假设双线性模型公式中 $W=U^{T} V,$ 原式可写为 $s\left(x_{i}, q\right)=x_{i}^{T} U^{T} V q=(U x)^{T}(V q),$ 分别对X和q进行线性<br>变换后计算点积, 相比与点积模型, 双线性模型在计算相似度时引入了非对称性。<br>加权平均：注意力分布 $\alpha_{i}$ 可以理解为在给定任务相关的查词q时，第i个输入向量受关注的程 度。 $a t t(X, q)=\sum_{i=1}^{N} \alpha_{i} x_{i}$<br><img src="/2020/09/06/attention/5.jpg" alt></li>
</ol>
<p>硬性注意力<br>软性注意力其选择的信息是所有输入信息在注意力分布下的期望。还有一种只关注到某一位置上的 信息，叫做硬性注意力(Hard Attention)。<br>选择最高概率的输入信息: $\quad a t t(X, q)=x_{j},$ j为概率最大的输入信息的下标，即</p>
<script type="math/tex; mode=display">
j=\operatorname{argmax} \alpha_{i}, i \in[1, N]</script><p>硬性注意力缺点是基于最大采样或随机采样的方式来选抨信息。因此最终的损失函数与注意力分布 之间的函数关系不可导，无法使用反向传播算法进行训练。为了使用反向传播算法，一般使用软性 注意力来代替硬性注意力。<br>键值对注意力<br>可以用键值对(key-value pair)格式来表示输入信息，其中 “键” 计算注意力分布 $\alpha_{i},$ “值”用来 计算聚合信息。<br>用 $(K, V)=\left[\left(k_{1}, v_{1}\right), \ldots,\left(k_{N}, v_{N}\right)\right]$ 表示N个输入信息，给定任务相关的查词向量q时, 注意力函数：</p>
<script type="math/tex; mode=display">
a t t((K, V), q)=\sum_{i=1}^{N} \alpha_{i} v_{i}=\sum_{i=1}^{N} \frac{\exp \left(s\left(k_{i}, q\right)\right)}{\sum_{j=1}^{N} \exp \left(s\left(k_{j}, q\right)\right)} v_{i}</script><p>其中 $s\left(k_{i}, q\right)$ 为打分函数.当 $K=V$ 时，鍵值对模式就等价于普通的注意力机制。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>课程</tag>
        <tag>进阶</tag>
      </tags>
  </entry>
  <entry>
    <title>自编码器(待更新)</title>
    <url>/2020/10/09/autodecoder/</url>
    <content><![CDATA[<p>暂时断更，不定期更新，看心情~</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>python实现bp神经网络</title>
    <url>/2020/08/31/bp-program/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇blog是基于python的bp神经网络代码实现，本身没有借助任何包，主要是想对该算法进行一个深入了解。基于马疝病数据集的一个二分类，结构相对简单，准确性不是很高，但是对神经网络的深入理解有很大的启发</p>
<a id="more"></a>
<h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">用户手册——by clever_bobo</span></span><br><span class="line"><span class="string">本函数是基于bp神经网络的二分类项目，(输出层激活函数为sigmod）是最基本的3层网络，暂时没有设置纠错选项，请谨慎输入</span></span><br><span class="line"><span class="string">多分类，输出层激活函数请使用线性整流函数ReLU或者不设置激活函数</span></span><br><span class="line"><span class="string">输入项分别为训练数据集，模型数据集，隐藏层节点数量，训练次数，学习率</span></span><br><span class="line"><span class="string">为方便显示，特加入tqdm模块，加个显示条</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">学习随笔：</span></span><br><span class="line"><span class="string">目前的主要问题：</span></span><br><span class="line"><span class="string">bp神经网络核心还是基于梯度下降法优化的，但是此时损失函数是非凸的，也就是说局部最小不一定是全局最小，有可能会到一个跟全局最小差别很大的局部最小点或者鞍点</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">梯度消失问题（鞍点？激活函数引起的尤其是sigmod，tanh）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">计算时间长（目前大部分人都吐槽的问题）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">优化（本代码均未使用）：</span></span><br><span class="line"><span class="string">初始化一般随机化参数，满足均值为0的正态分布，方差一般取2/(N+M)，N为上一层节点数量，M为下一层节点数量</span></span><br><span class="line"><span class="string">随机梯度下降增加动量（加速收敛，减少震荡）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#读取数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Readdata</span>(<span class="params">filename</span>):</span></span><br><span class="line">	<span class="comment">#创建指向文件的指针，用于打开文件</span></span><br><span class="line">	fp=open(filename)</span><br><span class="line">	<span class="comment">#创建放置数据和标签的空列表</span></span><br><span class="line">	dataset,datalabel=[],[]</span><br><span class="line">	<span class="comment">#读取文件</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> fp.readlines():</span><br><span class="line">		<span class="comment">#文件按行读取，每一行存为一个字符串，根据数据记录格式对其进行切割</span></span><br><span class="line">		databuff=i.strip().split()</span><br><span class="line">		<span class="comment">#将数据和标签（每行最后一个）分别放入对应的列表，这里使用的是append函数，千万不要用extend函数</span></span><br><span class="line">		dataset.append([float(j) <span class="keyword">for</span> j <span class="keyword">in</span> databuff[:<span class="number">-1</span>]])</span><br><span class="line">		datalabel.append(float(databuff[<span class="number">-1</span>]))</span><br><span class="line">	<span class="comment">#返回读取的数据和标签</span></span><br><span class="line">	<span class="keyword">return</span> dataset,datalabel</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置参数,本代码使用的是3层神经网络，暂时不支持多层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Para_Init</span>(<span class="params">input,hidden=<span class="number">3</span>,output=<span class="number">3</span></span>):</span></span><br><span class="line">	<span class="comment">#设置隐藏层单元的权重，参数大小服从标准正态分布</span></span><br><span class="line">	input_hidden=np.random.randn(input,hidden)</span><br><span class="line">	<span class="comment">#设置输出单元的权重</span></span><br><span class="line">	hidden_output=np.random.randn(hidden,output)</span><br><span class="line">	<span class="comment">#设置隐藏层的偏置值</span></span><br><span class="line">	hidden_bias=np.random.randn(<span class="number">1</span>,hidden)</span><br><span class="line">	<span class="comment">#设置输出层的偏置值</span></span><br><span class="line">	output_bias=np.random.randn(<span class="number">1</span>,output)</span><br><span class="line">	<span class="comment">#返回参数</span></span><br><span class="line">	<span class="keyword">return</span> input_hidden,hidden_output,hidden_bias,output_bias</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置激活函数，本函数使用的是sigmod函数，该函数主要用于二分类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmod</span>(<span class="params">z</span>):</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置sigmod函数的微分项,后面对参数纠正时候会用到</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">D_sigmod</span>(<span class="params">z</span>):</span></span><br><span class="line">	<span class="keyword">return</span> np.multiply(z,(<span class="number">1</span>-z))</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练函数，用于训练参数，参数设置依次为数据，标签，权重，偏置,学习率</span></span><br><span class="line"><span class="comment"># 核心算法在这部分，纠正参数部分看不懂就自己推导一遍公式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Train</span>(<span class="params">dataset,datalabel,input_hidden,hidden_output,hidden_bias,output_bias,learn_rate=<span class="number">0.01</span></span>):</span></span><br><span class="line">	<span class="comment">#for循环就是</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(len(datalabel)):</span><br><span class="line">		<span class="comment">#设置输入值</span></span><br><span class="line">		input_value=np.mat(dataset[i])</span><br><span class="line">        <span class="comment">#设置样本中的标签值</span></span><br><span class="line">		output_label=np.mat(datalabel[i])</span><br><span class="line">		<span class="comment">#计算隐藏层的单元值得大小</span></span><br><span class="line">		hidden_value=sigmod(np.dot(input_value,input_hidden)-hidden_bias)</span><br><span class="line">		<span class="comment">#计算神经网络的输出，因为本样本是二分类，所以采用sigmod激活</span></span><br><span class="line">		output_value=sigmod(np.dot(hidden_value,hidden_output)-output_bias)</span><br><span class="line">		<span class="comment">#更新参数</span></span><br><span class="line">		<span class="comment">#函数求导</span></span><br><span class="line">		d_output_value=D_sigmod(output_value)</span><br><span class="line">		<span class="comment">#计算误差</span></span><br><span class="line">		error=output_label-output_value</span><br><span class="line">		<span class="comment">#计算输出层层权重+偏置更新量</span></span><br><span class="line">		hidden_output_change=learn_rate*np.dot(np.transpose(hidden_value),np.multiply(error,d_output_value))</span><br><span class="line">		output_bias_change=-learn_rate*np.multiply(error,d_output_value)</span><br><span class="line">		<span class="comment">#更新隐藏层权重+偏置</span></span><br><span class="line">		d_hidden_value=D_sigmod(hidden_value)</span><br><span class="line">		t2=np.transpose(np.dot(hidden_output,np.multiply(error,d_output_value)))</span><br><span class="line">		input_hidden_change=learn_rate*np.dot(np.transpose(input_value),np.multiply(t2,d_hidden_value))</span><br><span class="line">		hidden_bias_change=-learn_rate*t2</span><br><span class="line">		<span class="comment">#数值更新</span></span><br><span class="line">		hidden_bias +=hidden_bias_change</span><br><span class="line">		hidden_output +=hidden_output_change</span><br><span class="line">		input_hidden +=input_hidden_change</span><br><span class="line">		output_bias += output_bias_change</span><br><span class="line">	<span class="keyword">return</span> hidden_bias,hidden_output,input_hidden,output_bias</span><br><span class="line"></span><br><span class="line"><span class="comment">#测试模型的准确程度(本次测试仅限于二分类的情况，多分类在输出层激活函数不要用sigmod函数)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Testing</span>(<span class="params">dataset,datalabel,hidden_bias,hidden_output,input_hidden,output_bias</span>):</span></span><br><span class="line">	rightnum=<span class="number">0</span></span><br><span class="line">	<span class="comment">#num0,num1=0,0</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(len(dataset)):</span><br><span class="line">		input_value=np.mat(dataset[i])</span><br><span class="line">		output_label=np.mat(datalabel[i])</span><br><span class="line">		hidden_value=sigmod(np.dot(input_value,input_hidden)-hidden_bias)</span><br><span class="line">		output_value=sigmod(np.dot(hidden_value,hidden_output)-output_bias)</span><br><span class="line">		<span class="keyword">if</span> abs(output_value-output_label)&lt;<span class="number">0.5</span>:</span><br><span class="line">			print(<span class="string">&quot;本次判断正确！模型判断为%d，实际为%d&quot;</span>%(datalabel[i],datalabel[i]))</span><br><span class="line">			rightnum += <span class="number">1</span></span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			t=abs(datalabel[i]<span class="number">-1.0</span>)</span><br><span class="line">			print(<span class="string">&quot;失败了失败了！模型判断为%d，实际为%d&quot;</span>%(t,datalabel[i]))</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> rightnum*<span class="number">1.0</span>/len(datalabel)		</span><br><span class="line"><span class="keyword">if</span>  __name__ ==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">	dataset, datalabel = Readdata(<span class="string">r&#x27;C:\Users\97751\Desktop\BP神经网络(马疝病数据集)\horseColicTraining.txt&#x27;</span>)</span><br><span class="line">	<span class="comment">#datatest,labeltest =Readdata(r&quot;C:\Users\97751\Desktop\BP神经网络(马疝病数据集)\horseColicTest.txt&quot;)</span></span><br><span class="line">	hidden=int(input(<span class="string">&quot;神经网络设置多少个隐藏节点:&quot;</span>))</span><br><span class="line">	input_hidden,hidden_output,hidden_bias,output_bias=Para_Init(len(dataset[<span class="number">0</span>]),hidden,output=<span class="number">1</span>)</span><br><span class="line">	time=int(input(<span class="string">&quot;训练次数？&quot;</span>))</span><br><span class="line">	learn_rate=float(input(<span class="string">&quot;学习率？&quot;</span>))</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(time)):</span><br><span class="line">		hidden_bias,hidden_output,input_hidden,output_bias=Train(dataset,datalabel,input_hidden,hidden_output,hidden_bias,output_bias,learn_rate)</span><br><span class="line">	<span class="comment">#error=Testing(datatest,labeltest,hidden_bias,hidden_output,input_hidden,output_bias)</span></span><br><span class="line">	error=Testing(dataset, datalabel,hidden_bias,hidden_output,input_hidden,output_bias)</span><br><span class="line">	print(<span class="string">&quot;正确率为：%.4f&quot;</span>%(error))</span><br></pre></td></tr></table></figure>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>本代码的准确率大概在80%左右，不算高，没有任何优化，纯粹作为理解算法原理使用，使用的数据集暂时放在百度云盘上（以后可能会直接放在github仓库），链接如下：<br>链接：<a href="https://pan.baidu.com/s/17vhkmKcf-C_qDREX7Fujjw">https://pan.baidu.com/s/17vhkmKcf-C_qDREX7Fujjw</a>  提取码：tgha<br>如若想要真正应用建议使用当前的python的机器学习库，里面集成了很多算法模型！</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>实例</tag>
      </tags>
  </entry>
  <entry>
    <title>如何快速下载github文件</title>
    <url>/2020/10/22/download-github/</url>
    <content><![CDATA[<h2 id="快速下载github文件方法如下："><a href="#快速下载github文件方法如下：" class="headerlink" title="快速下载github文件方法如下："></a>快速下载github文件方法如下：</h2><ul>
<li><p>打开github 页面，复制 <code>Download ZIP</code> 的链接地址，如下图所示<br><img src="/2020/10/22/download-github/1.PNG" alt></p>
</li>
<li><p>打开下载网站 <a href="https://d.serctl.com/?dl_start">https://d.serctl.com/?dl_start</a> ，将链接粘贴上去</p>
</li>
<li><p>等待网站下载完成，然后选择你的github文件，点击下载地址即可<br><img src="/2020/10/22/download-github/2.PNG" alt></p>
</li>
</ul>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>图卷积神经网络(待更新)</title>
    <url>/2020/10/09/gcn/</url>
    <content><![CDATA[<p>暂时不想更新，先留个坑位，以后看心情更新~</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>安装 pydensecrf 安装包</title>
    <url>/2020/10/22/install-pydensecrf/</url>
    <content><![CDATA[<h2 id="安装报错如下："><a href="#安装报错如下：" class="headerlink" title="安装报错如下："></a>安装报错如下：</h2><p><img src="/2020/10/22/install-pydensecrf/1.PNG" alt></p>
<h2 id="解决方法如下："><a href="#解决方法如下：" class="headerlink" title="解决方法如下："></a>解决方法如下：</h2><ul>
<li><p>首先，先安装包cython <code>pip install cython</code></p>
</li>
<li><p>离线下载pydensecrf的安装包，下载地址：<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#pydensecrf">https://www.lfd.uci.edu/~gohlke/pythonlibs/#pydensecrf</a> ，找到对应的版本！</p>
</li>
<li><p>离线安装 例如<code>pip install C:\Users\97751\Downloads\pydensecrf-1.0rc2-cp38-cp38-win_amd64.whl</code></p>
</li>
</ul>
<p>大功告成，安装成功~</p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>通俗理解卷积神经网络</title>
    <url>/2020/08/31/cnn/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>卷积神经网络主要是对图片进行处理，比如说人脸识别等等。该种算法参考了人眼视觉的某些特性，利用某些特殊的卷积核对图像进行处理分割。下面，我将对卷积神经网络的来源，架构，优化，优劣以及应用等方面进行简单介绍。</p>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在2012年，卷积神经网络第一次正式投入使用，Alex Krizhevsky曾使用该网络赢得2012年的图像识别竞赛，并将该网络错误率从0.26降到0.15，这在当时是一个非常巨大的进步。目前，卷积神经网络仍是一种非常流行深度学习算法，该算法使用特殊的卷积核对图片进行处理，提取一些高维特征，然后对这些高维特征的处理以达到我们的目的，所以这种方法用于处理图像方面的问题事非常有效！<br><a id="more"></a></p>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>对于图像处理问题，首先我们先举一个基础例子：图像分类！简单来说，图像分类就是输入一个图像，输出该图像中物品的类别或者说分到每个类的概率大小，以下图为例：<br><img src="/2020/08/31/cnn/1.PNG" alt><br>当输入图像如上图所示，我们希望算法能够告诉我们这是狗，或者80%的概率是狗。<br>对于人类而言，对于物品的识别能力是与生俱来的，我们天生就能够识别物品，并且这项能力会伴随着我们的能力逐渐增长，因此我们往往不需要过多的思考就能够辨别出我们所处的环境与位置。所以，我们在辨识物品的时候，与神经网络算法不同，不需要特意的对物品进行打标签，因为这项能力主要来源于我们的知识储备以及对于不同图像环境的学习能力，而这些对于机器而言都是不具备的，因此我们就想要提出一种方法，使得机器具备学习并识别图像的能力，CNN算法因此就被提了出来！</p>
<h2 id="电脑能看见什么"><a href="#电脑能看见什么" class="headerlink" title="电脑能看见什么"></a>电脑能看见什么</h2><p>当电脑看见一幅图像时，它看到的是一系列像素值，举个例子来说吧，电脑可能读取一个32<em>32</em>3的数字矩阵（分别对应RGB），矩阵的大小由图片的大小与分辨率决定。现在我们来具体举一个实际的例子方便理解，假如我们有一个JPG格式的彩色照片，其大小是480×480，那么我们可以用用480×480×3的矩阵来表示该向量。其中，矩阵中每个数字都是在0~255之间，用于描述该点的像素在RGB上的灰度大小。这些数字对于我们人类在对图像分类的时候毫无意义，但是对于电脑而言却是非常有用，电脑判别图像的方式就是对输入的数字矩阵进行相关的处理，最终输出结果！</p>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><p>根据上面的描述我们知道，电脑看到的图像就是一系列数字矩阵，那么我们应该如何处理才能得到我们想要的结果呢？首先，我们希望电脑能够对所有照片进行分析，找到它们之间的不同的点，接下来我们再进行更深一步的操作，即找到这些照片独特的特点，比如说对于一个大象的照片，算法可以找到其长长的鼻子，然后我们根据这样的一个或或者多个特征，对该图像进行分类。真正实际应用时，我们对图片中的边缘与曲线信息这样的低维特征进行提取，通过一系列卷积层来获得更加抽象的高维特征，这就是卷积神经网络的主要原理，下面我们来探讨下算法中的细节问题！</p>
<h2 id="卷积神经网络的启发"><a href="#卷积神经网络的启发" class="headerlink" title="卷积神经网络的启发"></a>卷积神经网络的启发</h2><p>卷积神经网络受生物研究中视皮层启发得到，在人眼的视皮层中，有一小部分细胞对看见事物中的特殊部分非常敏感。这个观点由Hubel和 Wiesel在1962年的一个有趣的实验中得到论证，他们证明了在大脑中有部分独立的细胞仅对视野中某些确定事物的轮廓有反应。比如说有一些神经细胞仅在看到了垂直的轮廓时兴奋激活，而有一些其他的神经细胞则只会在看到水平的轮廓时激活。之所以会产生这样的现象，是因为这些特殊的神经细胞都是由一种柱状结构构成，对视觉效果十分敏感，于是我们就提出能够在我们的算法中，也设置类似这样的一种特殊结构，用于提取图片中的信息呢？这就是卷积神经网络的起源。</p>
<h2 id="CNN的结构"><a href="#CNN的结构" class="headerlink" title="CNN的结构"></a>CNN的结构</h2><p>现在，我们回到正题上，CNN到底是如何具体的处理图像的呢？下面我先简单的描述下：输入图像—&gt;卷积—&gt;非线性变换—&gt;池化—&gt;全连接层—&gt;输出。根据前面的描述可知，输出可以是单个的类，也可以是最能描述图像的类的概率。接下来，我们将重点介绍每一层的作用。<br><img src="/2020/08/31/cnn/7.PNG" alt></p>
<h3 id="第一层：卷积层"><a href="#第一层：卷积层" class="headerlink" title="第一层：卷积层"></a>第一层：卷积层</h3><p>在卷积神经网络网络中，第一层网络我们一般都设置为卷积层，在该层我们将进行如下操作：输入图像，例如对神经网络输入一个32×32×3的一个数字矩阵，然后对其进行卷积操作。对于卷积操作，我举一个例子来简单解释一下吧，如下图所示：<br><img src="/2020/08/31/cnn/2.PNG" alt><br>假设有一个手电筒，它的照射范围是5×5，我们先令其照在左上角，如上图的左边所示，，然后令手电向右滑动，遍历图像所有的部分。在神经网络中，我们称这样的“手电筒”为卷积核，其实就是一个数字矩阵；“手电筒”的照射范围就是这个数字矩阵的维度！值得注意的一点是，卷积核的深度要与图像的深度一致，比如说图片是32×32×3的矩阵，那么卷积核就要是n×n×3的矩阵。仍以上图为例，演示下图像是如何卷积的：刚开始卷积核位于左上角，卷积核上的每一个数字与图像中对应的位置的数字相乘然后求和，这样我们就可以得到在该位置处卷积的到的结果，然后滑动卷积核，使其遍历到图像上的所有位置，这样我们就可以得到图像卷积后的结果。例如，对于32×32×3的图像我们采用5×5×3的卷积核进行卷积，可以得到28×28×3的矩阵，该矩阵就是我们卷积后得到的结果。<br>根据上述描述，我可以知道，我们设置了一个卷积核对图像进行遍历操作，得到一个卷积后的结果，这个结果可以帮助我们检测图像中是否存在某些特征，因此我们往往称卷积核为特征识别器。所谓特征，在这里我指的是直线轮廓，曲线轮廓，纯色区域这样的特征，因为这些特征时所有照片都具备的简单特征。下面我来据图介绍下，卷积核是如何筛选特征的，假设存在某一7×7×3的卷积核，如下图所示：<br><img src="/2020/08/31/cnn/3.PNG" alt><br>为了简化我们的问题，我们先只考虑其中的一个7×7的卷积核，如图所示，该卷积核在沿着曲线的方向上具有更高的数值！我们将该卷积核应用于下面图像的左上角，具体计算方法在上面已经说过，因此不再赘述。<br><img src="/2020/08/31/cnn/4.PNG" alt><br>图片上对应位置与卷积核相乘求和：<br><img src="/2020/08/31/cnn/5.PNG" alt><br>最终得到结果为 6600，这是一个比较大的一个数值，因为卷积核卷积的区域的形状与卷积核非常类似！接下来，我们移动卷积核，如下图所示，在进行卷积操作：<br><img src="/2020/08/31/cnn/6.PNG" alt><br>此时卷积结果就会变得非常小，为0，这是因为该区域与卷积核的形状差距巨大！因此卷积核能够起到一种提取特征的效果，如果对应区域的图像与卷积核类似得到数值就会较大，否则就会很小（最小一般为0），在本次演示中，使用的卷积核是一种曲线检测器，检测图片中是否有类似的曲线。在对图片进行卷积操作时，我们可以选用多个卷积核对图片进行处理，以得到多个低级特征，再对低级特征进行卷积就可以提取高级特征，比如检测图片中有没有大象的鼻子。一般来说，使用的卷积核越多，深度越深，其效果就会越强。</p>
<h3 id="其它层"><a href="#其它层" class="headerlink" title="其它层"></a>其它层</h3><p>为了拓展CNN以及提高其鲁棒性，我们往往在CNN中添加一些非线性变换（激活函数）以及池化操作，这两个不是我们讲解的重点，因此只是简单的描述下其功能，非线性变换拓展神经网络的功能，使其适应性更强，能够应用于更多的问题；池化能够降低神经网络中的复杂度，有效减少训练时间以及避免神经网络出现过拟合！<br>利用前面的一些操作，我们可以检测到图片中高级特征，而在最后一层我们往往使用全连接层的方式连接，其作用如下：<br>在这一层，其输入往往是一个n×m的矩阵，输出一个N维向量，其中N为图像可能的类别种类。例如，对于手写数字的识别，N应该是10，因为有10位数字。这个N维向量中的每一个数字都表示某个类的概率。举个例子,如果结果向量为[0,0.1,0.1,0.75,0,0,0,0,0,0.05],那么这个图片，有10%的概率是1；有10%的概率为2；有75%的概率为3；有5%的概率为9。<br><img src="/2020/08/31/cnn/8.PNG" alt></p>
<h2 id="卷积神经网络的训练"><a href="#卷积神经网络的训练" class="headerlink" title="卷积神经网络的训练"></a>卷积神经网络的训练</h2><p>卷积神经网络是一种监督学习算法，其训练方式与一般的神经网络类似，核心是bp算法，在这里做简单的介绍“在刚刚开始训练的时候，我们将神经网络所有的权重与卷积核随机初始化，将数据钱箱传递，根据损失函数将误差后向传播，以一个常用的损失函数为例：MSE（均方误差）</p>
<script type="math/tex; mode=display">E_{\text {total}}=\sum \frac{1}{2}(\text {target}-\text {output})^{2}</script><p>利用bp算法更新卷积核与权重的值，使其损失函数较小达到算法的最优容量。<br>ps:反向传播时会把卷积核和图片展开，然后就同全连接的反向传播一样训练了！</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>如何安装 torch-sparse</title>
    <url>/2020/10/22/install-torch-sparse/</url>
    <content><![CDATA[<h2 id="安装torch-sparse报错，解决方法如下："><a href="#安装torch-sparse报错，解决方法如下：" class="headerlink" title="安装torch-sparse报错，解决方法如下："></a>安装torch-sparse报错，解决方法如下：</h2><ul>
<li>安装torch-sparse需要对应的依赖关系，请参考官网： <a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html">https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html</a></li>
</ul>
<p>例如<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install torch-scatter&#x3D;&#x3D;latest+cu102 -f https:&#x2F;&#x2F;pytorch-geometric.com&#x2F;whl&#x2F;torch-1.5.0.html</span><br><span class="line">pip install torch-sparse&#x3D;&#x3D;latest+cu102 -f https:&#x2F;&#x2F;pytorch-geometric.com&#x2F;whl&#x2F;torch-1.5.0.html</span><br><span class="line">pip install torch-cluster&#x3D;&#x3D;latest+cu102 -f https:&#x2F;&#x2F;pytorch-geometric.com&#x2F;whl&#x2F;torch-1.5.0.html</span><br><span class="line">pip install torch-spline-conv&#x3D;&#x3D;latest+cu102 -f https:&#x2F;&#x2F;pytorch-geometric.com&#x2F;whl&#x2F;torch-1.5.0.html</span><br><span class="line">pip install torch-geometric</span><br></pre></td></tr></table></figure></p>
<p>注意，选择好对应的版本应该就没有什么大问题了~</p>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>长短期记忆(待更新)</title>
    <url>/2020/10/09/lstm/</url>
    <content><![CDATA[<p>暂时断更~</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown基本语法一</title>
    <url>/2020/08/26/markdown1/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇blog主要面向初步接触markdown的小白，主要是面向windows用户，简单介绍markdown语法规则，同时方便自己在忘记时能快速查看~<br>本文参考文献：<br><a href="https://hyxxsfwy.github.io/2016/01/15/Hexo-Markdown-%E7%AE%80%E6%98%8E%E8%AF%AD%E6%B3%95%E6%89%8B%E5%86%8C">一、简明语法手册</a><br><a href="https://www.mereith.com/2018/12/08/markdown%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/">二、某知名哲学家的blog</a><br><a href="https://www.jianshu.com/p/191d1e21f7ed/">三、Markdown基本语法</a><br><a href="https://www.jianshu.com/p/599857933f6e">四、Markdown基本语法总结</a></p>
<h2 id="创建新文章"><a href="#创建新文章" class="headerlink" title="创建新文章"></a>创建新文章</h2><p>第一种：打开powershell（管理员），打开到对应位置的目录，输入以下代码<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new <span class="string">&quot;新文章的名字&quot;</span></span><br></pre></td></tr></table></figure><br>第二种：打开blog文章所在的位置，例如<code>D:\Program Files\hexo-blog\myblog\source\_posts</code>,创建txt文件，更改拓展名为.md</p>
<p>两种方法没有什么本质区别，效率也是一样的，大家可以根据自己的习惯选择,个人推荐选择第一种~</p>
<h2 id="创建blog文件的文件头部分"><a href="#创建blog文件的文件头部分" class="headerlink" title="创建blog文件的文件头部分"></a>创建blog文件的文件头部分</h2><p>每一篇blog一般都要包含4个部分，以本篇为例：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">title: markdown基本语法一</span><br><span class="line">date: 2020-08-26 23:00:00</span><br><span class="line">tags: [markdown,入门] </span><br><span class="line">categories: markdown</span><br><span class="line">description: 本篇blog主要面向初步接触markdown的小白，主要是面向windows用户，简单介绍markdown语法规则，同时方便自己在忘记时能快速查看~</span><br></pre></td></tr></table></figure><br>分别对应本篇文章的标题，编辑时间，标签以及文章分类。另外，标签可以有多个，用英文逗号隔开；分类建议只有一个；简介只在首页列表显示，打开后不会显示，是可选项。<br><a id="more"></a></p>
<h2 id="编写文章的标题"><a href="#编写文章的标题" class="headerlink" title="编写文章的标题"></a>编写文章的标题</h2><p>markdown支持6级标题，以<code>#</code>作为关键字，识别标题，实例如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一级标题</span></span><br><span class="line"><span class="comment">## 二级标题</span></span><br><span class="line"><span class="comment">### 三级标题</span></span><br><span class="line"><span class="comment">#### 四级标题</span></span><br><span class="line"><span class="comment">##### 五级标题</span></span><br><span class="line"><span class="comment">###### 六级标题</span></span><br></pre></td></tr></table></figure><br>效果如下：<br><img src="https://pic.mereith.com/img/show_title.png-slim" alt></p>
<p>此外还有一些特殊的标题符号，例如在标题前加入<code>-</code>, <code>*</code>,<code>+</code>，可着重标记该项，效果如下：</p>
<ul>
<li>着重强调符号</li>
</ul>
<h2 id="字体设置"><a href="#字体设置" class="headerlink" title="字体设置"></a>字体设置</h2><h3 id="粗体和斜体"><a href="#粗体和斜体" class="headerlink" title="粗体和斜体"></a>粗体和斜体</h3><p>使用<code>*</code>和<code>**</code>分别表示斜体和粗体，格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">*斜体*，**粗体**，***斜体加粗***</span><br></pre></td></tr></table></figure><br>效果展示：<em>斜体</em>,<strong>粗体</strong>,<strong><em>斜体加粗</em></strong></p>
<h2 id="字体、字号、颜色"><a href="#字体、字号、颜色" class="headerlink" title="字体、字号、颜色"></a>字体、字号、颜色</h2><p>使用关键字，可以指定字体的颜色和大小，其格式为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">指定字体类型： &lt;font face=<span class="string">&quot;黑体&quot;</span>&gt;我是黑体字&lt;/font&gt;</span><br><span class="line">指定字体大小： &lt;font size=12&gt;我是12号字&lt;/font&gt;</span><br><span class="line">指定字体颜色：&lt;font color=<span class="comment">#0099ff&gt;我是蓝色字&lt;/font&gt; #0099ff 为颜色的16进制代码</span></span><br><span class="line">指定字体颜色、字号、字体类型&lt;font color=<span class="comment">#0099ff size=12 face=&quot;黑体&quot;&gt;黑体&lt;/font&gt;</span></span><br></pre></td></tr></table></figure><br>效果如下：<br>指定字体类型： <font face="黑体">我是黑体字</font><br>指定字体大小： <font size="12">我是12号字</font><br>指定字体颜色：<font color="#0099ff">我是蓝色字</font> #0099ff 为颜色的16进制代码<br>指定字体颜色、字号、字体类型<font color="#0099ff" size="12" face="黑体">黑体</font></p>
<h3 id="换行"><a href="#换行" class="headerlink" title="换行"></a>换行</h3><p>方法1：连敲2个以上空格+enter键；<br>方法2：利用html语法，<code>&lt;br&gt;</code>。</p>
<h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><p>单独一行使用<code>***</code>或者<code>---</code>，表示该行作为分割线</p>
<hr>
<h3 id="删除线"><a href="#删除线" class="headerlink" title="删除线"></a>删除线</h3><p>删除线是指在原文本上画一条线，类似在纸上写错了划线来删除，一般用于表示过时的版本或者错误的写法较为醒目，格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">~~删除线~~</span><br></pre></td></tr></table></figure><br>效果：<del>删除线</del></p>
<h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><p>在编写blog的时候，往往会参考一些网站，我们需要把网站链接放在正文中，此时就需要用到超链接；此外还可以放一些图片链接<br>网站链接格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[链接说明](链接地址)</span><br></pre></td></tr></table></figure><br>例如：<a href="https://www.mereith.com/">某知名哲学家♂的个人主页</a></p>
<p>图片超链接格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">![图片说明](图片地址)</span><br></pre></td></tr></table></figure><br>例如：<img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=543966357,2530364137&amp;fm=26&amp;gp=0.jpg" alt="某知名老婆的图片" title="蕾姆"></p>
<h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><p>可以用<code>\</code>来表示注释，也就是说<code>\</code>后的文字不会被转义，格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">\<span class="comment">#标题格式，但不是标题</span></span><br></pre></td></tr></table></figure><br>具体效果如下：#标题格式，但不是标题</p>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p>使用<code>&gt;</code>来表示文字的引用，往往在引用参考文献中的语句时使用，其格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; Sow nothing, reap nothing</span><br></pre></td></tr></table></figure><br>此外，引用还可以嵌套使用，例如:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; 这是第一个引用</span><br><span class="line">&gt;&gt; 这是第一个引用中的引用</span><br><span class="line">&gt;&gt;&gt; 这是第一个引用中的引用的引用</span><br></pre></td></tr></table></figure><br>效果展示如下：</p>
<blockquote>
<p>这是第一个引用</p>
<blockquote>
<p>这是第一个引用中的引用</p>
<blockquote>
<p>这是第一个引用中的引用的引用</p>
</blockquote>
</blockquote>
</blockquote>
<p>我就不套娃了，大家自己可以试试~</p>
<h3 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h3><p><strong>首先</strong>，是行内代码块，使用一对 ` 来括住文字，格式如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">行内`代码`块</span><br></pre></td></tr></table></figure><br>其效果如下：行内<code>代码</code>块</p>
<p><strong>其次</strong>，多行代码块，使用一对```来括住文字，效果如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一行</span><br><span class="line">第二行</span><br><span class="line">第三行</span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<p><strong>最后</strong>，支持规定语言的代码展示，以python为例，在 ``` 后加入编码格式即可<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">&quot;hello world&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="引用本地图片"><a href="#引用本地图片" class="headerlink" title="引用本地图片"></a>引用本地图片</h2><p>在hexo中，如果你想引用本地图片，最好先安装hexo-asset-image的插件，保证图片不会因为移动而丢失。安装完该插件之后，每次产生新的.md文件的同时，还会在生成与之同名的文件夹。将想要上传的图片传入该文件夹，按照<code>![你想输入的替代文字](xxxx/图片名.jpg)</code>引用即可，效果如下：<br><img src="/2020/08/26/markdown1/test1.jpg" alt="测试图片"></p>
]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>入门</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>深度理解逻辑回归</title>
    <url>/2020/08/27/log-regression/</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>&emsp;&emsp;所谓机器学习，听起来很高深莫测，实际上并不复杂，简单来说就是数据驱动的算法。而数据驱动就是指在给定的数据情况下，我们需要找到一种合适的算法对这些数据进行操作，从而实现我们预期的目标，具体我们需要做的任务就是找到合适的模型来描述输入到输出之间的映射关系，然后在使用优化的方法不断对模型中的参数进行优化，使得最后得到的结果鲁棒性最高。在上述的描述中，主要涉及两个方面：1.合适的模型 2.合适的优化算法。其实，我们在初中就接触过相关的领域，比如给定一系列坐标点（x，y），利用线性回归公式（最小二乘法）拟合曲线y=ax+b。其中，y=ax+b就是我们选择的模型，而最小二乘法就是对其参数a，b的优化算法。当然，这只是最简单的应用，实际机器学习的算法往往要比这个复杂很多，但究其本质还是一样的。<br>&emsp;&emsp;对于机器学习而言，其功能非常强大，能完成分类、回归、转录、机器翻译、异常与检测、合成和采样等等。其中，分类与回归是机器学习的最基本的两项功能。上述举的例子就是回归算法。而今天，我主要介绍一种常用的分类算法——逻辑回归。对于分类的而言，逻辑回归是一种重要的学习方式，该方法所假设的函数在经过优化后的到模型适用性非常高。另外，之所以该方法在名称中带有回归二字，是因为其算法原理与线性回归之间有很深的联系，在下面介绍算法原理的时候我会重点讲解。</p>
<a id="more"></a>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>&emsp;&emsp;分类，顾名思义，就是将输入的数据分为不同的类别，其结果是离散的，比如预测明天的天气是去晴天还是非晴天。在计算机中，我们往往使用不同的数字代表不用的类别，比如1代表晴天，0代表非晴天。<br>&emsp;&emsp;但是，如果算法仅仅只能够告诉我们分类的结果是非常不精准的，我们更期望算法能够告诉我们发生某种情况的概率，比如明天晴天的概率为80%，非晴天的概率为20%，这样的话我们的可操作性就会更强，可以人为的添加参数（优化）对算法进行矫正。仍以晴天为例，如果分类算法较为精准，我们可以用50%为阈值，如果晴天的概率大于50%，就是晴天；如果算法计算晴天概率比实际偏大，那我们可以设置60%为阈值，大于60%为晴天</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>&emsp;&emsp;在正式讲解逻辑回归的算法之间，首先我们需要了解一下线性回归的基本原理。我们假定输出y由输入x线性决定（这里的x,y都是向量)，其表达式为：</p>
<script type="math/tex; mode=display">f(x)=\theta^{T} x······（1）</script><p>当输入变量只有一个时，就变成了我们熟悉的y=ax+b，此时，利用最小二乘法，具体的效果大致如下：<br><img src="/2020/08/27/log-regression/1.png" alt="y=ax+b"><br>&emsp;&emsp;当多个变量的时候，就由一维向量到多维，例如二维就会得到一个平面，而非一条直线了，其共同的特点就是模型是连续的，其值域为（-∞，+∞），这是线性回归模型的一个重要的特征。</p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>&emsp;&emsp;逻辑回归是一种回归算法，该种算法主要应用于二分类的状况，例如明天北京是否会下雪，一个人五年内是否会得心脏病等等，由线性回归延伸出来的，那么具体是如何出现的呢？下面我来具体说明，仍以晴天为例：</p>
<ol>
<li>对于明天天气如何，我们该如何预测呢？首先，对于给定输入，比如温度，云层厚度，时间，风力，这些参数在算法中表现为x1,x2,x3……，我们最容易想到的就是令预测概率p（x）为x的线性函数，这样就和线性回归一致了，其值域为（-∞，+∞），并不符合我们的要求（概率应该在0~1之间），因此我们需要对其进行改进，使其符合我们的要求；</li>
<li>如果我们需要限制值域，在机器学习中最常用的就是ln函数，因此我们做一个简单的调整，令lnp(x)为x的线性函数，也就是说p（x）=exp(ax+b)，但是该函数无论正负，均只能在一个方向上约束值域，因此还需要改进；</li>
<li>最后，对lnp作简单的调整，令其在两个方向都被约束，我们用的方法是逻辑转换，令ln(p/1-p)为x的线性函数，那么p（x）的值域就是[0,1]<br>&emsp;&emsp;因此逻辑回归的表达式为（在印刷体中我们往往都采用log代表ln）：<script type="math/tex; mode=display">\log \frac{p(x)}{1-p(x)}=\beta_{0}+x \beta··········（2）</script></li>
</ol>
<p>如（2）中所示，p（x）为事情发生的概率，令$\log \frac{p(x)}{1-p(x)}$成为x的线性函数，解得p(x)为：</p>
<script type="math/tex; mode=display">\mathrm{p}\left(x ; \beta_{0}, \beta\right)=\frac{1}{1+e^{-\left(\beta_{0}+x \beta\right)}}··········（3）</script><p>（3）式相比于（2）式更容易理解条件概率p（x），但（2）式更能凸显逻辑回归与线性回归之间的关系。<br>如图所示：<br><img src="/2020/08/27/log-regression/2.png" alt="逻辑回归"><br>该图为函数逻辑回归曲线的大致形状，基本完成了我们期望的条件：曲线较为平滑，值域位于[0,1]。</p>
<h2 id="主要特征"><a href="#主要特征" class="headerlink" title="主要特征"></a>主要特征</h2><ol>
<li>一般取$\beta_{0}+x \quad \beta=0$为分类的边界，那么如果x是一维，那么分类边界就是一个点（类似在数轴上分类）；如果是二维分类边界就是一条直线，以此类推。之所以如此设置，是因为设置β0+ x β= 0为边界后，我们可以认为当算法输出的概率p（x）≥0.5时，分类结果 Y为1,；当p&lt;0.5时，分类结果Y为0。或者说算法输入的$\beta_{0}+x \quad \beta≥0$时，分类结果为1；$\beta_{0}+x \quad \beta&lt;0$时，分类结果为0（目前只考虑二分类的情况，多种分类情况在后面有介绍）。这样我们就可以把前面计算得到的概率转换为分类的结果了，既得到了分类的条件概率，又得到了分类的结果。</li>
<li>逻辑回归计算得到的条件概率是由数据点到边界之间的距离决定的，为$\frac{\beta_{0}+x \beta}{|\beta|}$。也就是说如果距离边界远，那么为1（或者0）的概率就会越大。另外这个公式也说明了当||β||越大时，在同一数据集下，分类得到的概率会更像极端（0,1）靠近，如下图所示<br><img src="/2020/08/27/log-regression/3.png" alt="逻辑回归"><br>（备注：最后一种是用线性分类的方式进行分类的）</li>
<li>逻辑回归是跟据线性回归演化而来，提出时间早，科学家们对其研究较为透彻，运用较为熟练。此外，该算法较为简单，且适用性较强，机器学习的算法在准确度够得情况下，尽量选择较为简单的算法，避免出现过拟合</li>
</ol>
<h2 id="对于多分类的逻辑回归"><a href="#对于多分类的逻辑回归" class="headerlink" title="对于多分类的逻辑回归"></a>对于多分类的逻辑回归</h2><p>逻辑回归本质上是对二分类的事件进行的分类，但是对于多分类的情况，我们可以通过建立多个分类模型，将多分类差分成多个二分类的情况，在利用逻辑回归进行分类，如图所示：<br><img src="/2020/08/27/log-regression/4.png" alt="逻辑回归"><br>一般而言如果存在n中情况，我们会设置n个分类器，每个分类器对其中一种进行识别，其条件概率的计算公式为：</p>
<script type="math/tex; mode=display">P(Y=c \mid X=x)=\frac{e^{\beta_{0}^{(c)}+x \beta^{(c)}}}{\sum_{i=1}^{n} e^{\beta_{0}^{(j)}+x \beta^{(i)}}}</script><p>判别方法与之前类似，如果超过0.5就认为是第c类。值得一提是，当只有两种情况时，令$\beta^{0}=\beta_{0}^{(1)}-\beta_{0}^{(0)}$以及$\beta=\beta^{(1)}-\beta^{(0)}$时，多情况的逻辑分类就转换成二分类的情况了。<br><br>下面是我自己的推导过程：<br><img src="/2020/08/27/log-regression/5.png" alt="逻辑回归"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown进阶心得（更新中）</title>
    <url>/2020/08/27/markdown2/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇blog主要是在编辑markdown时，遇到了很多问题，同时也找到了很多技巧，所以在这里mark一下</p>
<h2 id="首行缩进"><a href="#首行缩进" class="headerlink" title="首行缩进"></a>首行缩进</h2><p>英文字符空格 <code>&amp;ensp;</code><br>中文字符空格 <code>&amp;emsp;</code><br>不断行的空白格 <code>&amp;nbsp;</code><br>其中较为常用的是<code>&amp;emsp;</code>,其效果如下：<br>&emsp;&emsp;<strong>首行缩进两字符</strong><br><a id="more"></a></p>
<h2 id="文字居中"><a href="#文字居中" class="headerlink" title="文字居中"></a>文字居中</h2><p>文字居中使用<code>&lt;center&gt; ``&lt;/center&gt;</code>来括住文字，格式如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;center&gt; 文字居中&lt;&#x2F;center&gt; </span><br></pre></td></tr></table></figure><br>效果展示：</p>
<center> 文字居中</center> 

<h2 id="插入公式"><a href="#插入公式" class="headerlink" title="插入公式"></a>插入公式</h2><p>参考链接：(<a href="https://www.jianshu.com/p/7ab21c7f0674">https://www.jianshu.com/p/7ab21c7f0674</a>)<br>按照参考链接里面的教程，将冲突的配置文件更改，即可在exo中渲染MathJax数学公式<br><br>但是该公式有严格的LaTeX语法，语法规则参考<a href="https://www.jianshu.com/p/25f0139637b7">《markdown中公式编辑教程》</a>,所以较为复杂，在这里推荐一种懒人方法：使用“mathpix”插件，官方的下载地址为(<a href="https://mathpix.com/)。">https://mathpix.com/)。</a><br>使用方法浅显易懂，注册账号登录，截屏选取公式，复制结果。如果是编辑公式，我们可以在word上编写或者写在纸上，用该软件转换格式，实测非常好用~</p>
<h2 id="writage插件"><a href="#writage插件" class="headerlink" title="writage插件"></a>writage插件</h2><p>官方地址：(<a href="https://www.writage.com">https://www.writage.com</a>)<br>使用教程：(<a href="https://www.cnblogs.com/craigtaylor/p/13540170.html">https://www.cnblogs.com/craigtaylor/p/13540170.html</a>)<br>该插件可以将word文件转换为.md文件，但是经过我的实测，并不是很理想，待开发中~</p>
<h2 id="插入pdf链接"><a href="#插入pdf链接" class="headerlink" title="插入pdf链接"></a>插入pdf链接</h2><p>安装插件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install --save hexo-pdf</span><br></pre></td></tr></table></figure><br>之后就可以在新建的md文件中插入pdf链接了，格式如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">外部链接：</span><br><span class="line">&#123;% pdf 链接地址 %&#125;</span><br><span class="line">本地连接：</span><br><span class="line">&#123;% pdf .&#x2F;pdf名字.pdf %&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>引用文献时，注意被引用的语句与上下段落之间空出一行</p>
<center> <font size="12" color="#DC143C" face="黑体">未完待续~</font> </center>

]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>进阶</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>Jetson nano平台深度学习环境配置</title>
    <url>/2020/10/09/nano1/</url>
    <content><![CDATA[<p>&emsp;&emsp;最近从老师那里搞到了一个Jason nano开发板，本来里面有一个系统，然而断电导致系统文件损坏，使得系统无法正常启动，无限重启，我从各个网站看到的教程都不太行，只能把sd卡拔下来，重刷一遍系统，把里面的深度学习环境配置一下，中间遇到了很多坑，写个blog记录一下~</p>
<a id="more"></a>
<h2 id="烧写镜像系统"><a href="#烧写镜像系统" class="headerlink" title="烧写镜像系统"></a>烧写镜像系统</h2><p>参考网站：<a href="https://blog.csdn.net/beckhans/article/details/89136269?biz_id=102&amp;utm_term=jetson%20nano&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-89136269&amp;spm=1018.2118.3001.4187">玩转Jetson Nano（一）烧写系统</a><br>&emsp;&emsp;Jetson nano开发板是基于arm64架构，其源文件，镜像等文件与其他略有所不同，请大家注意！首先，需要烧写系统，需要的材料：</p>
<ol>
<li>电源线</li>
<li>SD卡，最低需要32G，但是越大越好，谁不喜欢大的呢？</li>
<li>Jetson nano开发板</li>
<li>无线网卡或者网线，个人推荐使用无线网卡，方便使用，但是无线网卡一般开发板不带，需要自己安装。</li>
<li>鼠标、键盘 </li>
<li>HDMI线，显示器</li>
</ol>
<p>安装系统大致需要上面那些东西，材料准备好就可以烧录系统了。</p>
<h3 id="下载镜像文件"><a href="#下载镜像文件" class="headerlink" title="下载镜像文件"></a>下载镜像文件</h3><p>英伟达官方地址：<a href="https://developer.nvidia.com/embedded/dlc/jetson-nano-dev-kit-sd-card-image">https://developer.nvidia.com/embedded/dlc/jetson-nano-dev-kit-sd-card-image</a><br>这个包大小大概在6G大小，下载时需要耐心等待~</p>
<h3 id="格式化SD卡"><a href="#格式化SD卡" class="headerlink" title="格式化SD卡"></a>格式化SD卡</h3><p>&emsp;&emsp;使用SD Card Formatter格式化SD卡，这个工具哪都有，百度一下下载一个，为方便使用，我把我下载的压缩包链接放上来吧<br>链接：<a href="https://pan.baidu.com/s/1SJWX9rpTmrZ4vrpDURtH0A">https://pan.baidu.com/s/1SJWX9rpTmrZ4vrpDURtH0A</a>   提取码：x9dk<br><img src="/2020/10/09/nano1/1.PNG" alt></p>
<p>傻瓜式操作，选择快速格式化，点击格式化即可</p>
<h3 id="烧写镜像"><a href="#烧写镜像" class="headerlink" title="烧写镜像"></a>烧写镜像</h3><p>&emsp;&emsp;Jetson nano推荐使用Etcher写入镜像，大概40分钟，但是具体时间还要看各自的CPU，静静等待镜像写入完毕即可，该软件的官方下载地址：<a href="https://www.balena.io/etcher/">https://www.balena.io/etcher/</a><br>&emsp;&emsp;我的云盘链接：<a href="https://pan.baidu.com/s/10pZZxllhLqPACCUErsl6ag">https://pan.baidu.com/s/10pZZxllhLqPACCUErsl6ag</a>  提取码：h4h2<br><img src="/2020/10/09/nano1/2.PNG" alt><br>选择镜像，烧写系统，等待~</p>
<h3 id="启动Jetson-nano开发板"><a href="#启动Jetson-nano开发板" class="headerlink" title="启动Jetson nano开发板"></a>启动Jetson nano开发板</h3><p>将SD卡插入Jetson Nano,开机，完成一些设置，时区，语言，输入法什么的，就不截图了，看见下图，大功告成<br><img src="/2020/10/09/nano1/3.png" alt></p>
<p>这样开发板的基本环境就配置的差不多了，下面来介绍一下如何<strong>对系统进行配置</strong></p>
<h2 id="安装无线网卡"><a href="#安装无线网卡" class="headerlink" title="安装无线网卡"></a>安装无线网卡</h2><p>&emsp;&emsp;我推荐我购买的这款无线网卡，下面是我购买时的链接，封面有如何安装无线网卡的教程，安装好之后，开机有提示，无线网卡已安装，点击设置即可<br>查看无线网IP地址<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ifconfig</span><br></pre></td></tr></table></figure><br><a href="https://item.m.jd.com/product/48830712399.html?wxa_abtest=o&amp;utm_user=plusmember&amp;gx=RnFmymVYbWDdmNRP--txCbnq4uODIu4zbc4S&amp;ad_od=share&amp;utm_source=androidapp&amp;utm_medium=appshare&amp;utm_campaign=t_335139774&amp;utm_term=CopyURL"><strong>点击链接</strong></a></p>
<h2 id="设置中文页面"><a href="#设置中文页面" class="headerlink" title="设置中文页面"></a>设置中文页面</h2><p>&emsp;&emsp;Jetson nano开发板自带中文环境(英文大佬可以忽略这一条)，需要简单配置一下，这边有一个小坑，需要注意。打开设置，如图所示的按钮<br><img src="/2020/10/09/nano1/4.PNG" alt></p>
<p>点击语言支持，会出现如下界面，不用检查更新语言支持，取消就行了<br><img src="/2020/10/09/nano1/5.PNG" alt></p>
<p>在菜单和窗口的语言选项栏中，将汉语（中国）鼠标左键选中往上面拖，拖到第一个<br><img src="/2020/10/09/nano1/6.PNG" alt></p>
<p>reboot一下，即可完成中文界面的设置，比较简单</p>
<h2 id="设置中文输入法"><a href="#设置中文输入法" class="headerlink" title="设置中文输入法"></a>设置中文输入法</h2><p>&emsp;&emsp;Jetson nano开发板并不带有中文输入法，需要自己安装。值得注意的一点是，Jetson Nano 是arm64结构，不是amd64结构，所以安装的软件需要适配arm64。我们比较常见的输入法有搜狗输入法、谷歌输入法，其中搜狗输入法不支持arm64架构，所以这里推荐使用谷歌输入法。<br>我们使用fcitx插件来支持中文输入法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install fcitx-googlepinyin fcitx-module-cloudpinyin fcitx-sunpinyin-y</span><br></pre></td></tr></table></figure><br>上述代码会将fcitx插件以及相关的中文输入法的包一并下载下来，安装完成后，点击在设置里面点击语言支持，选择fcitx选项，如图所示<br><img src="/2020/10/09/nano1/7.PNG" alt></p>
<p><strong>这里面需要注意一点，由于fcitx的兼容性问题，可能会导致输入中文时不显示候选框，这时需要删除掉多余的插件！</strong><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt remove fcitx-module-kimpanel</span><br></pre></td></tr></table></figure><br>这一点非常重要，最后在终端输入reboot，重启一下。重启之后，桌面右上角会出现一个小键盘，点击这个小键盘就可以设置输入法了<br><img src="/2020/10/09/nano1/8.PNG" alt><br>默认设置为英文输入，按’ctrl’+空格即可切换输入法</p>
<h2 id="更换镜像源"><a href="#更换镜像源" class="headerlink" title="更换镜像源"></a>更换镜像源</h2><p>&emsp;&emsp;NVIDIA官方提供的Linux镜像版本为Ubuntu 18.04 LTS，镜像默认的是Ubuntu官方源，在国内使用该源下载程序速度较慢，所以需要更换。<br><strong>首先，备份原来的源文件，这一点非常重要，备份之后不要随意更改备份项</strong><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak  </span><br></pre></td></tr></table></figure><br>然后，打开源的配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">gedit /etc/apt/sources.list</span><br></pre></td></tr></table></figure><br>删除原来的文件，添加一下以下内容(我使用的是ubuntu18.04，如果不是这个版本，可以在下面官网中选择合适的镜像源)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu-ports&#x2F; bionic main restricted universe multiverse</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu-ports&#x2F; bionic main restricted universe multiverse</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu-ports&#x2F; bionic-updates main restricted universe multiverse</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu-ports&#x2F; bionic-updates main restricted universe multiverse</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu-ports&#x2F; bionic-backports main restricted universe multiverse</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu-ports&#x2F; bionic-backports main restricted universe multiverse</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu-ports&#x2F; bionic-security main restricted universe multiverse</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu-ports&#x2F; bionic-security main restricted universe multiverse</span><br><span class="line"></span><br><span class="line"># 预发布软件源，不建议启用</span><br><span class="line"># deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu-ports&#x2F; bionic-proposed main restricted universe multiverse</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu-ports&#x2F; bionic-proposed main restricted universe multiverse</span><br></pre></td></tr></table></figure><br><strong>清华大学镜像源官网：</strong><a href="https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu-ports/">https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu-ports/</a></p>
<p>更新完毕后，更新镜像源<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></p>
<p>如果需要的话，可以更新软件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get upgrade</span><br></pre></td></tr></table></figure><br><strong>这里注意一点，我在实际进行操作时，更新镜像源时，报了几个错误，但是目前来说没有太大的影响，所以暂时忽略这一点。</strong></p>
<h2 id="安装python3-6"><a href="#安装python3-6" class="headerlink" title="安装python3.6"></a>安装python3.6</h2><p>&emsp;&emsp;安装官方的镜像系统时，内部自带了python2.7的环境，我们需要对其进行升级，升级到3.6版本，注意Jetson nano开发版最多支持python3.6<br>输入命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:jonathonf/python-3.6</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install python3.6</span><br></pre></td></tr></table></figure><br>此时还不能使用python3.6版本，因为默认使用的是python2.7，需要调整优先级<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --install /usr/bin/python python /usr/bin/python2 100</span><br><span class="line">sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 150</span><br></pre></td></tr></table></figure><br>如图所示：<br><img src="/2020/10/09/nano1/9.PNG" alt></p>
<h2 id="pip换源"><a href="#pip换源" class="headerlink" title="pip换源"></a>pip换源</h2><p>换源的目的是为了下载python包时，速度有极大提升<br>修改/创建文件<code>~/.pip/pip.conf</code>，内容如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line"> </span><br><span class="line">index-url &#x3D; https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple</span><br></pre></td></tr></table></figure></p>
<h2 id="配置tensorflow环境"><a href="#配置tensorflow环境" class="headerlink" title="配置tensorflow环境"></a>配置tensorflow环境</h2><p>在安装系统镜像的时候，默认安装了cuda，可以使用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nvcc -V</span><br></pre></td></tr></table></figure><br>查看CUDA的版本号(如果失败，尝试使用root权限登录试试)<br><img src="/2020/10/09/nano1/10.PNG" alt><br>如图所示，我所用的就是cuda10.2版本<br>接下来就是需要安装对应的依赖关系<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python3-pip libhdf5-serial-dev hdf5-tools</span><br></pre></td></tr></table></figure><br>接下来就是安装TensorFlow GPU版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip3 install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v42 tensorflow-gpu==1.13.1+nv19.3 --user</span><br></pre></td></tr></table></figure><br>这个安装比较慢，中间我大概花了有一个小时的时间，所以慢慢等待就行了</p>
<h2 id="设置远程连接"><a href="#设置远程连接" class="headerlink" title="设置远程连接"></a>设置远程连接</h2><p>这里推荐使用XShell实现SHH远程连接</p>
<ul>
<li>可以记录账号密码，连接后自动登录</li>
<li>可以为不同的主机设置个性化描述</li>
<li>可以同时打开一个主机的多个终端</li>
<li>可以同时打开不同主机的多个终端</li>
</ul>
<p>功能基本都简单易懂，就不再啰嗦了，直接百度找一个安装包就行了，其大致界面如下：<br><img src="/2020/10/09/nano1/11.PNG" alt></p>
<h2 id="设置远程文件传输"><a href="#设置远程文件传输" class="headerlink" title="设置远程文件传输"></a>设置远程文件传输</h2><p>远程文件传输推荐使用WinSCP图形化SHH工具，可以用于后续将模型文件从自己的电脑传输到Nano，非常方便。<br>WinSCP下载地址：<a href="https://www.onlinedown.net/soft/20088.htm">https://www.onlinedown.net/soft/20088.htm</a><br>安装完成后，直接输入上一步得到的Nano的IP、主机用户名、密码即可访问Nano的文件系统。<br><img src="/2020/10/09/nano1/12.PNG" alt><br>其使用界面如下：<br><img src="/2020/10/09/nano1/13.PNG" alt></p>
<h2 id="设置远程桌面控制"><a href="#设置远程桌面控制" class="headerlink" title="设置远程桌面控制"></a>设置远程桌面控制</h2><p>远程桌面控制推荐使用VNC远程控制，VNC的条件为：远程的ubuntu机器已经登录进入桌面，且已经开启vino-server，则在远程使用vnc－viewer可直接连接。因此，要做两件事：</p>
<ul>
<li>在Nano上安装vino-server</li>
<li>在Nano上启动vino-server<h3 id="打开Nano终端，依次执行"><a href="#打开Nano终端，依次执行" class="headerlink" title="打开Nano终端，依次执行"></a>打开Nano终端，依次执行</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install vino</span><br></pre></td></tr></table></figure>
<h3 id="设置VINO登录选项"><a href="#设置VINO登录选项" class="headerlink" title="设置VINO登录选项"></a>设置VINO登录选项</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">gsettings <span class="built_in">set</span> org.gnome.Vino prompt-enabled <span class="literal">false</span></span><br><span class="line">gsettings <span class="built_in">set</span> org.gnome.Vino require-encryption <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<h3 id="将网卡加入VINO服务"><a href="#将网卡加入VINO服务" class="headerlink" title="将网卡加入VINO服务"></a>将网卡加入VINO服务</h3>执行，查看网卡UUID<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nmcli connection show</span><br></pre></td></tr></table></figure>
<img src="/2020/10/09/nano1/14.PNG" alt><br>将UUID拷贝粘贴到如下命令的单引号[‘your UUID’]之间，并执行<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dconf write /org/gnome/settings-daemon/plugins/sharing/vino-server/enabled-connections <span class="string">&quot;[&#x27;your UUID&#x27;]&quot;</span></span><br><span class="line"><span class="built_in">export</span> DISPLAY=:0</span><br></pre></td></tr></table></figure>
<h3 id="在windows上安装VNC软件"><a href="#在windows上安装VNC软件" class="headerlink" title="在windows上安装VNC软件"></a>在windows上安装VNC软件</h3>在自己电脑的windows系统中安装VNC viewer<br>下载地址：<a href="https://www.realvnc.com/en/connect/download/viewer/">https://www.realvnc.com/en/connect/download/viewer/</a><br>当然，其他地方应该也可以下到绿色版。<h3 id="在Nano上启动vino-server"><a href="#在Nano上启动vino-server" class="headerlink" title="在Nano上启动vino-server"></a>在Nano上启动vino-server</h3>Nano上执行<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/lib/vino/vino-server</span><br></pre></td></tr></table></figure>
<img src="/2020/10/09/nano1/15.PNG" alt><br>本指令执行后，终端不关闭，在windows系统打开VNC viewer软件执行如下操作登录Nano：<br><img src="/2020/10/09/nano1/16.PNG" alt><br>第一次连接会弹出警告信息，点击[continue]，如果出现你的nano说明连接成功了<h3 id="添加开机启动vino-server"><a href="#添加开机启动vino-server" class="headerlink" title="添加开机启动vino-server"></a>添加开机启动vino-server</h3>我们希望每次Nano上电开机后，自动启动vino-server，只需要将上述命令加入开启启动选项，这样就彻底接键盘显示器的麻烦了。下面快来设置吧！<br>在终端输入命令：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo gnome-session-properties</span><br></pre></td></tr></table></figure>
打开Startup Applications应用，添加如下开机启动内容</li>
<li>名称：StartVinoServer</li>
<li>指令：/usr/lib/vino/vino-server<br><img src="/2020/10/09/nano1/17.PNG" alt><br>重启Nano，无需再次设置，直接就可以连接远程桌面控制<br>实际测试，该种方法延迟较高，不是非常好用</li>
</ul>
<h2 id="安装vscode"><a href="#安装vscode" class="headerlink" title="安装vscode"></a>安装vscode</h2><p>安装命令如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget --content-disposition https://packagecloud.io/headmelted/codebuilds/packages/debian/stretch/code-oss_1.45.0-1586135971_arm64.deb/download.deb</span><br><span class="line">sudo dpkg -i code-oss_1.45.0-1586135971_arm64.deb</span><br></pre></td></tr></table></figure><br>但是实际测试过程中，该种方法下载速度特别慢，因此我把我下载的包放在我的云盘上，需要自取<br>链接：<a href="https://pan.baidu.com/s/16-qsyBzOw2kvcgY3FTNkKQ">https://pan.baidu.com/s/16-qsyBzOw2kvcgY3FTNkKQ</a>  提取码：xwgo<br>利用文件传输winscp将该包传输到nano上，打开对应文件目录，执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i code-oss_1.45.0-1586135971_arm64.deb</span><br></pre></td></tr></table></figure><br>或者，可以利用Ubuntu上面自带的文件浏览器，打开到文件所在的位置，点击鼠标右键安装<br>但是，安装完毕后，还是不能直接使用，因为我们需要指定vscode的工作目录，因此我在我的家目录创建了一个vscode文件夹<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir /home/bobo/vscode</span><br></pre></td></tr></table></figure></p>
<p>打开vscode，在终端输入<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># open vscode</span></span><br><span class="line"><span class="built_in">alias</span> vscode=<span class="string">&#x27;code-oss . --user-data-dir=/home/bobo/vscode&#x27;</span></span><br></pre></td></tr></table></figure><br>此时，这么长的语句不好记住，因此我们可以创建一个别名，这里我用的是root权限创建的别名<br>在终端输入<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># open vscode</span></span><br><span class="line">gedit ~/.bashrc</span><br></pre></td></tr></table></figure><br>将<code>alias vscode=&#39;code-oss . --user-data-dir=/home/bobo/vscode&#39;</code>添加到该文件的末尾，并保存退出。<br><img src="/2020/10/09/nano1/18.PNG" alt><br>重启，再次打开终端，注意的一点，因为我用的是root权限，所以需要用root权限使用该命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vscode</span><br></pre></td></tr></table></figure><br>显示如下界面：<br><img src="/2020/10/09/nano1/19.PNG" alt><br>说明别名设置成功，大致环境配置的就差不多了，以后还有什么缺的再补充！</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>实践</tag>
        <tag>Jason_nano</tag>
      </tags>
  </entry>
  <entry>
    <title>序列对序列（Seq2Seq）（待更新）</title>
    <url>/2020/10/09/seq2seq/</url>
    <content><![CDATA[<p>暂时不更新，留个坑位，要是有人催更就更，估计也没人看~</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>通俗理解梯度下降法</title>
    <url>/2020/08/28/sgd/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>机器学习的本质就是模型+优化，本篇blog主要是简单介绍一种优化算法——梯度下降法。相比于其他的优化算法，该算法的适用性较高，尤其在深度网络的学习中，主流的优化算法就是梯度下降法。虽然说该方法适用性高，但是仍有一定的局限性，比如对于非凸函数收敛于局部最小，梯度消失，梯度爆炸等问题，但总的来说该算法的应用还是十分广泛的。</p>
<h2 id="场景假设"><a href="#场景假设" class="headerlink" title="场景假设"></a>场景假设</h2><p>&emsp;&emsp;首先，我们先假设一个场景：一个人被放在山上，现在他想找到一条下山的路到达山脚，但是这个人没有地图，也不知道所处位置和方向。另外山上还起了大雾，导致能见度很低，因此没有办法直接找到一条合适的路径，只能自己一步步摸索，那么这个时候，便可利用梯度下降算法来帮助自己下山。<br><img src="/2020/08/28/sgd/1.jpg" alt><br>&emsp;&emsp;具体怎么做呢，首先以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着下降方向走一步，然后又继续以当前位置为基准，再找最陡峭的地方，再走直到最后到达最低处。虽然这么走不一定是最短路径，但是每一步都能保证自己离山脚更进一步，这就是梯度下降法的核心：一步步慢慢的靠近最小值点，不一定最快，但一定有效。那么一定会有同学问了，如果我想找到f(x)的最高点怎么办？同样，我们可以利用梯度下降的方法找-f(x)最小点即可。</p>
<a id="more"></a>
<h2 id="数学基础知识回顾"><a href="#数学基础知识回顾" class="headerlink" title="数学基础知识回顾"></a>数学基础知识回顾</h2><h3 id="一元函数的导数与泰勒展开"><a href="#一元函数的导数与泰勒展开" class="headerlink" title="一元函数的导数与泰勒展开"></a>一元函数的导数与泰勒展开</h3><p>&emsp;&emsp;在微积分中, 函数 $f(x)$ 在点 $x_{0}$ 上的导数定义为:</p>
<script type="math/tex; mode=display">\quad f^{\prime}\left(x_{0}\right)=\lim _{x \rightarrow x_{0}} \frac{f(x)-f\left(x_{0}\right)}{x-x_{0}}</script><p>&emsp;&emsp;它在几何上指的就是函数 $f(x)$ 在 $x_{0}$ 上的切线方向.通常来说，为了计算某个函数 $f(x)$ 的最大值或者最小值，通常都会计算它的导数 $f^{\prime}(x)$ ，然后求解方程 $f^{\prime}(x)=0$ 就可以得到函数的临界点，进一步判断这些临界点是否是最大值或者最 小值。但是临界点并不一定是全局最大值或者全局最小值，基至不是局部的最大值或者局部最小值。<br><br>例如：函数 $f(x)=x^{3},$ 它的导数是 $f^{\prime}(x)=3 x^{2},$ 因此 $x=0$ 是它的临界点。但 $x=0$ 则不是这个函数的局部最大值或者局部最小值点, 因为 $f(x)<0, \forall x<0$ 且 $f(x)>0, \forall x&gt;0$。从 Taylor 级数的角度来看, $\quad f(x)$ 在 $x_{0}$ 附近的 Taylor 级数是</0,></p>
<script type="math/tex; mode=display">
f(x)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\frac{f^{\prime \prime}\left(x_{0}\right)}{2}\left(x-x_{0}\right)^{2}+O\left(\left|x-x_{0}\right|^{3}\right)</script><p>&emsp;&emsp;对于临界点 $x_{0}$ 而言，它满足条件 $f^{\prime}\left(x_{0}\right)=0$ 。当 $f^{\prime \prime}\left(x_{0}\right)&gt;0$ 时, 可以得到 $x_{0}$ 是 $f(x)$ 的局部最小值;当$f^{\prime \prime}\left(x_{0}\right)&lt;0$时, 可以得到 $x_{0}$ 是 $f(x)$ 的局部最大值。而对于上面的例子 $f(x)=x^{3}$ 而言，临界点 0 的二阶导数则是 $f^{\prime \prime}(0)=0$, 因此使用上面的方法则无法判断临界点 0 是否是局部极值。</p>
<h3 id="多元函数的梯度与泰勒展开"><a href="#多元函数的梯度与泰勒展开" class="headerlink" title="多元函数的梯度与泰勒展开"></a>多元函数的梯度与泰勒展开</h3><p>&emsp;&emsp;对于多元函数 $f(\mathbf{x})=f\left(x_{1}, \cdots, x_{n}\right)$ 而言，同样可以计算它们的”导数”，也就是偏导数和梯度。i.e. 它的梯度可以定义为：</p>
<script type="math/tex; mode=display">
\nabla f(\mathbf{x})=\left(\frac{\partial f}{\partial x_{1}}(\mathbf{x}), \cdots, \frac{\partial f}{\partial x_{n}}(\mathbf{x})\right)</script><p>而多元函数 $f(\mathbf{x})$ 在点 $\mathbf{x}_{0}$ 上的 Taylor 级数是：</p>
<script type="math/tex; mode=display">f(\mathbf{x})=f\left(\mathbf{x}_{0}\right)+\nabla f\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}_{0}\right)^{T} H\left(\mathbf{x}-\mathbf{x}_{0}\right)+O\left(\left|\mathbf{x}-\mathbf{x}_{0}\right|^{3}\right)</script><p>其中 $H$ 表示 Hessian 矩阵。如果 $x_{0}$ 是临界点，并且 Hessian 矩阵是正定矩阵的时候, $f(\mathbf{x})$ 在 $x_{0}$ 处达到局部极小值。</p>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>&emsp;&emsp;从数学上的角度来看，梯度的方向是函数增长速度最快的方向，那么梯度的反方向就是函数减少最快的方向。那么，如果想计算一个函数的最小值，就可以使用梯度下降法的思想来做。<br>&emsp;&emsp;假设希望求解目标函数 $f(\mathbf{x})=f\left(x_{1}, \cdots, x_{n}\right)$ 的最小值, 可以从一个初始点 $\mathbf{x}^{(0)}=\left(x_{1}^{(0)}, \cdots, x_{n}^{(0)}\right)$ 开始, 基于学习率 $\eta&gt;0$ 构建一个迭代过程：当 $i \geq 0$ 时</p>
<script type="math/tex; mode=display">
x_{1}^{(i+1)}=x_{1}^{(i)}-\eta \cdot \frac{\partial f}{\partial x_{1}}\left(\mathbf{x}^{(i)}\right)</script><script type="math/tex; mode=display">
··········</script><script type="math/tex; mode=display">
x_{n}^{(i+1)}=x_{n}^{(i)}-\eta \cdot \frac{\partial f}{\partial x_{n}}\left(\mathbf{x}^{(i)}\right)</script><p>其中 $\mathbf{x}^{(i)}=\left(x_{1}^{(i)}, \cdots, x_{n}^{(i)}\right),$ 一旦达到收禽条件的话, 迭代就结束。<br><br>&emsp;&emsp;从梯度下降法的迭代公式来看, 下一个点的选择与当前点的位置和它的梯度相关。反之，如果要计算函数 $f(\mathbf{x})=f\left(x_{1}, \cdots, x_{n}\right)$ 的最大值, 沿着梯度的反方向前进即可,也就是说：</p>
<script type="math/tex; mode=display">x_{1}^{(i+1)}=x_{1}^{(i)}+\eta \cdot \frac{\partial f}{\partial x_{1}}\left(\mathbf{x}^{(i)}\right)</script><script type="math/tex; mode=display">··········</script><script type="math/tex; mode=display">x_{n}^{(i+1)}=x_{n}^{(i)}+\eta \cdot \frac{\partial f}{\partial x_{n}}\left(\mathbf{x}^{(i)}\right)</script><p>其中, $\mathbf{x}^{(i)}=\left(x_{1}^{(i)}, \cdots, x_{n}^{(i)}\right)$ 。整体来看，无论是计算函数的最大值或者最小值，都需要构建 一个迭代关系 $g$ ，那就是：</p>
<script type="math/tex; mode=display">\mathbf{x}^{(0)} \stackrel{g}{\longrightarrow} \mathbf{x}^{(1)} \stackrel{g}{\longrightarrow} \mathbf{x}^{(2)} \stackrel{g}{\longrightarrow} \cdots</script><p>也就是说对于所有的 $i \geq 0,$ 都满足迭代关系 $x^{(i+1)}=g\left(x^{(i)}\right)$ 。所以, 在以上的两个方法 中，我们可以写出函数 g 的表达式为：</p>
<script type="math/tex; mode=display">g(\mathbf{x})=\left\{\begin{array}{ll}
\mathbf{x}-\eta \nabla f(\mathbf{x}) & \text { 梯度下降法 } \\
\mathbf{x}+\eta \nabla f(\mathbf{x}) & \text { 梯度上升法 }
\end{array}\right.</script><p>但在实际应用时，当我们求解最大值时，可以对函数取反，求其最小值。</p>
<h2 id="实例测试"><a href="#实例测试" class="headerlink" title="实例测试"></a>实例测试</h2><h3 id="单变量函数的梯度下降法"><a href="#单变量函数的梯度下降法" class="headerlink" title="单变量函数的梯度下降法"></a>单变量函数的梯度下降法</h3><p>在这么我们举一个简单的例子，对于目标函数：</p>
<script type="math/tex; mode=display">J(\theta)=\theta^{2}</script><p>对函数进行微分，直接求导就可以得到</p>
<script type="math/tex; mode=display">J^{\prime}(\theta)=2 \theta</script><p>初始化，也就是起点，起点可以随意的设置，这里设置为</p>
<script type="math/tex; mode=display">\theta^{0}=1</script><p>学习率也可以随意的设置，这里设置为0.4</p>
<script type="math/tex; mode=display">\alpha=0.4</script><p>根据梯度下降的计算公式</p>
<script type="math/tex; mode=display">\Theta^{1}=\Theta^{0}-a \nabla J(\Theta) \quad 初始值为\Theta^{0}</script><p>我们开始进行梯度下降的迭代计算过程:</p>
<script type="math/tex; mode=display">\begin{aligned} \theta^{0} &=1 \\ \theta^{1} &=\theta^{0}-\alpha * J^{\prime}\left(\theta^{0}\right) \\ &=1-0.4 * 2 \\ &=0.2 \\ \theta^{2} &=\theta^{1}-\alpha * J^{\prime}\left(\theta^{1}\right) \\ &=0.04 \\ \theta^{3} &=0.008 \\ \theta^{4} &=0.0016 \end{aligned}</script><p>在上面的计算过程中，初始参数$\theta^{0}$，学习率$\alpha$我们是随意选择的，但因为本次的例子较为简单，所以效果还不错。但在实际应用中，目标函数往往会十分复杂，对于初始值的选择也十分重要，在后面我会介绍如何选取合适的初始值。<br><img src="/2020/08/28/sgd/2.png" alt></p>
<h3 id="多变量函数的梯度下降"><a href="#多变量函数的梯度下降" class="headerlink" title="多变量函数的梯度下降"></a>多变量函数的梯度下降</h3><p>我们假设有一个目标函数</p>
<script type="math/tex; mode=display">
J(\Theta)=\theta_{1}^{2}+\theta_{2}^{2}</script><p>现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从 梯度下降算法开始一步步计算到这个最小值！ 我们假设初始的起点为：</p>
<script type="math/tex; mode=display">\Theta^{0}=(1,3)</script><p>初始的学习率为：</p>
<script type="math/tex; mode=display">\alpha=0.1</script><p>函数的梯度为：</p>
<script type="math/tex; mode=display">\nabla J(\Theta)=\left\langle 2 \theta_{1}, 2 \theta_{2}\right\rangle</script><p>进行多次迭代：</p>
<script type="math/tex; mode=display">\begin{aligned}
\Theta^{0} &=(1,3) \\
\Theta^{1} &=\Theta^{0}-\alpha \nabla J(\Theta) \\
&=(1,3)-0.1(2,6) \\
&=(0.8,2.4) \\
\Theta^{2} &=(0.8,2.4)-0.1(1.6,4.8) \\
&=(0.64,1.92) \\
\Theta^{3} &=(0.512,1.536) \\
\Theta^{4} &=(0.4096,1.2288000000000001) \\
\vdots & \\
\Theta^{10} &=(0.10737418240000003,0.32212254720000005) \\
\vdots & \\
\Theta^{50} &=\left(1.1417981541647683 \mathrm{e}^{-05}, 3.425394462494306 \mathrm{e}^{-05}\right) \\
\vdots & \\
\Theta^{100} &=\left(1.6296287810675902 \mathrm{e}^{-10}, 4.888886343202771 \mathrm{e}^{-10}\right)
\end{aligned}</script><p><img src="/2020/08/28/sgd/3.png" alt></p>
<h2 id="梯度下降法的种类"><a href="#梯度下降法的种类" class="headerlink" title="梯度下降法的种类"></a>梯度下降法的种类</h2><h3 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h3><p>&emsp;&emsp;这是梯度下降法刚刚提出时的主流类型，但是现在已经不经常使用了，因为这种优化算法会使用整个数据集去计算代价函数的梯度去更新参数，如果数据集非常的大，批量梯度下降法会很慢，而且数据量这么大往往无法载入内存，但好处就是，在每次迭代完成之后能够保证每次的优化都是有效的。在随机初始化参数后，按如下方式计算代价函数的梯度，直到数据收敛：</p>
<script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}</script><p><img src="/2020/08/28/sgd/4.png" alt><br>上图是每次迭代后的等高线图，每个不同颜色的线表示代价函数不同的值。运用梯度下降会快速收敛到圆心，即唯一的一个全局最小值。</p>
<h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>&emsp;&emsp;相比于批量梯度下降法，随机梯度下降法具有更快的计算速度。首先，随机梯度下降的第一步是随机化整个数据集。在每次迭代仅选择其中一部分训练样本去计算代价函数的梯度，然后更新参数。即使是大规模数据集，随机梯度下降法也会很快收敛。由于数据点可能存在的误差，随机梯度下降得到结果的准确性可能不会是最好的，但是计算结果的速度很快。具体的计算方式如下：<br><img src="/2020/08/28/sgd/5.png" alt><br>其中，m代表训练样本的数量，如下图所示，随机梯度下降法不像批量梯度下降法那样收敛，而是游走到接近全局最小值的区域终止：<br><img src="/2020/08/28/sgd/6.png" alt></p>
<h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p>&emsp;&emsp;小批量梯度下降法是最广泛使用的一种算法，该算法每次随机使用m个训练样本（称之为一批）进行训练，能够更快得出准确的答案。小批量梯度下降法不是使用完整数据集，在每次迭代中仅使用m个训练样本去计算代价函数的梯度。一般小批量梯度下降法所选取的样本数量在50到256个之间，视具体应用而定，具体的计算方式如下：<br><img src="/2020/08/28/sgd/7.png" alt><br>其中，b表示一批训练样本的个数，m是训练样本的总数。</p>
<h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h2><h3 id="非凸函数局部收敛"><a href="#非凸函数局部收敛" class="headerlink" title="非凸函数局部收敛"></a>非凸函数局部收敛</h3><p>如果想要利用梯度下降法找到目标函数的全局最小值，那么该函数必须是凸函数<a href="https://zhuanlan.zhihu.com/p/51127402">(参考链接：什么是凸函数)</a>,如果不是凸函数，那么利用梯度下降法就有可能收敛于局部最小值，如下图所示<br><img src="/2020/08/28/sgd/9.png" alt><br>函数最终会收敛在上面的点，而非我们需要的最小值点！但是在实际应用中，很多目标函数都是非凸函数，尤其是神经网络，那么我们该如何解决这个问题呢？据我所知，当前还没有完美的解决方案，一般采用合适的学习率或者搭配一些其他的优化算法，使得计算出来的结果（可能是局部最优解）仍具有良好的效果，从而用该值作为我们优化算法的最优值，可能在算法上讲的并不是很清楚，有点像是黑盒子，但事实证明确实很有效。</p>
<h3 id="学习率选取不合适"><a href="#学习率选取不合适" class="headerlink" title="学习率选取不合适"></a>学习率选取不合适</h3><p>在本文的演示的例子中，因为目标函数较为简单，所以在随便拿了一个学习率后，仍然有不错的效果，但是实际应用时，如果学习率选取不当，会造成严重的后果，如下图所示：<br><img src="/2020/08/28/sgd/8.png" alt><br>如果学习率选取过大，那么计算结果就会在收敛点处震荡，而无法收敛到最小值；如果学习率选的过小，那么计算时间就会过长，占用大量资源，同时还有可能陷入局部最小而无法跳出。</p>
<h3 id="梯度爆炸与梯度消失"><a href="#梯度爆炸与梯度消失" class="headerlink" title="梯度爆炸与梯度消失"></a>梯度爆炸与梯度消失</h3><p>&emsp;&emsp;梯度爆炸与梯度消失是神经网络经常出现的一个问题，但究其本质还是梯度下降法的局限性，在这里简单介绍下，后面涉及到神经网络时再具体讲解。对函数求导数是基于链式，而链式法则是一个连乘的形式，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的梯度值接近0或特别大，也就是梯度消失或爆炸。梯度消失或梯度爆炸在本质原理上其实是一样的。<br>&emsp;&emsp;梯度爆炸将导致参数大小会出现很大的震荡，无法收敛；梯度消失将导致优化算法失效。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>目前主流的梯度下降法是小批量梯度下降法（ps:可能部分的算法称之为sgd）,每次均利用部分的数据对函数进行优化，从而快速的达到收敛。<br><img src="/2020/08/28/sgd/10.png" alt><br>如图所示，该种算法每次迭代并不是最有效的，因此我们可以对其增设“动量”。<br>回顾一下梯度下降法每次的参数更新公式:</p>
<script type="math/tex; mode=display">
\begin{aligned}
W &:=W-\alpha \nabla W \\
b &:=b-\alpha \nabla b
\end{aligned}</script><p>可以看到，每次更新仅与当前梯度值相关, 并不涉及之前的梯度。而动量梯度下降法则对各个mini-batch求得的梯度 $\nabla W, \nabla b$ 使用指数加权平均得到 $V_{\nabla w}, V_{\nabla b_{\bullet}}$ 并使用新的参数更新之前的参数。<br>例如，在100次梯度下降中求得的梯度序列为:</p>
<script type="math/tex; mode=display">
\left\{\nabla W_{1}, \nabla W_{2}, \nabla W_{3} \ldots \ldots \ldots \nabla W_{99}, \nabla W_{100}\right\}</script><p>则其对应的动量梯度分别为：</p>
<script type="math/tex; mode=display">\begin{array}{c}
V_{\nabla W_{0}}=0 \\
V_{\nabla W_{1}}=\beta V_{\nabla W_{0}}+(1-\beta) \nabla W_{1} \\
V_{\nabla W_{2}}=\beta V_{\nabla W_{1}}+(1-\beta) \nabla W_{2} \\
·········· \\
V_{\nabla W_{100}}=\beta V_{\nabla W_{99}}+(1-\beta) \nabla W_{100}
\end{array}</script><p>使用指数加权平均之后梯度代替原梯度进行参数更新。因为每个指数加权平均后的梯度含有之前梯度的信息, 动量梯度下降法因此得名。其效果如下：<br><img src="/2020/08/28/sgd/11.png" alt><br>简单的来说，增设动量之后，将优化时的左右的摆动减小，让目标函数收敛的更快。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>循环神经网络(暂时断更)</title>
    <url>/2020/10/09/rnn/</url>
    <content><![CDATA[<p>不定期更新~</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>基于SVD的图片分解</title>
    <url>/2020/08/28/svd-program/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇python基于numpy包实现SVD，因为有现成的API，所以直接调用即可。本代码实现对图片的奇异值分解效果展示</p>
<a id="more"></a>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">用户手册-by clever_bobo</span></span><br><span class="line"><span class="string">本函数用于测试奇异矩阵对图片进行分解，暂未设置任何纠错设置，请按照对应参数进行输入</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#读取文件，方块图像与风景图像</span></span><br><span class="line">im_square=mpimg.imread(<span class="string">r&quot;C:\Users\97751\Desktop\SVD\test1.png&quot;</span>)</span><br><span class="line">im_nature=mpimg.imread(<span class="string">r&quot;C:\Users\97751\Desktop\SVD\test2.png&quot;</span>)</span><br><span class="line"><span class="comment">#读取图片大小</span></span><br><span class="line">s1,s2,s3=im_square.shape</span><br><span class="line">n1,n2,n3=im_nature.shape</span><br><span class="line"><span class="comment">#转换格式</span></span><br><span class="line">im_square_temp = im_square.reshape(s1,s2 * s3)</span><br><span class="line">im_nature_temp = im_nature.reshape(n1, n2 * n3)</span><br><span class="line"><span class="comment">#调用numpy库中的线性函数的库</span></span><br><span class="line">U1,S1,VT1=np.linalg.svd(im_square_temp)</span><br><span class="line">U2,S2,VT2=np.linalg.svd(im_nature_temp)</span><br><span class="line"><span class="comment">#设置标志位</span></span><br><span class="line">flag=<span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> flag!=<span class="number">1</span>:</span><br><span class="line">    <span class="comment">#设置精度</span></span><br><span class="line">    nums=int(input(<span class="string">&quot;请输入保留的多少位奇异值：&quot;</span>))   </span><br><span class="line">    <span class="comment">#对方块图形进行SVD分解</span></span><br><span class="line">    img_square = (U1[:,<span class="number">0</span>:nums]).dot(np.diag(S1[<span class="number">0</span>:nums])).dot(VT1[<span class="number">0</span>:nums,:])</span><br><span class="line">    img_square= img_square.reshape(s1, s2 , s3)</span><br><span class="line">    <span class="comment">#对风景图形进行SVD分解</span></span><br><span class="line">    img_nature = (U2[:,<span class="number">0</span>:nums]).dot(np.diag(S2[<span class="number">0</span>:nums])).dot(VT2[<span class="number">0</span>:nums,:])</span><br><span class="line">    img_nature= img_nature.reshape(n1, n2 , n3)</span><br><span class="line">    <span class="comment">#精度分析</span></span><br><span class="line">    error1,error2=sum(S1[<span class="number">0</span>:nums])/sum(S1)*<span class="number">10000</span>//<span class="number">1</span>*<span class="number">0.01</span>,sum(S2[<span class="number">0</span>:nums])/sum(S2)*<span class="number">10000</span>//<span class="number">1</span>*<span class="number">0.01</span></span><br><span class="line">    <span class="comment">#画图部分</span></span><br><span class="line">    fig,ax=plt.subplots(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="comment">#原始方块输出</span></span><br><span class="line">    ax[<span class="number">0</span>][<span class="number">0</span>].imshow(im_square)</span><br><span class="line">    ax[<span class="number">0</span>][<span class="number">0</span>].set(title = <span class="string">&quot;original square&quot;</span>)</span><br><span class="line">    <span class="comment">#SVD方块输出</span></span><br><span class="line">    ax[<span class="number">0</span>][<span class="number">1</span>].imshow(img_square)</span><br><span class="line">    ax[<span class="number">0</span>][<span class="number">1</span>].set(title = <span class="string">&quot; SVD&#x27;s square  precision=&quot;</span>+str(error1)+<span class="string">&#x27;%&#x27;</span>)</span><br><span class="line">    <span class="comment">#原始山脉输出</span></span><br><span class="line">    ax[<span class="number">1</span>][<span class="number">0</span>].imshow(im_nature)</span><br><span class="line">    ax[<span class="number">1</span>][<span class="number">0</span>].set(title = <span class="string">&quot;original nature&quot;</span>)</span><br><span class="line">    <span class="comment">#SVD山脉输出</span></span><br><span class="line">    ax[<span class="number">1</span>][<span class="number">1</span>].imshow(img_nature)</span><br><span class="line">    ax[<span class="number">1</span>][<span class="number">1</span>].set(title = <span class="string">&quot; SVD&#x27;s nature precision=&quot;</span>+str(error2)+<span class="string">&#x27;%&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    flag=int(input(<span class="string">&quot;是否结束SVD分解？是请输入1: &quot;</span>))</span><br><span class="line">print(<span class="string">&quot;感谢使用测试——by clever_bobo!!!&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><p>保留1位奇异值：<br><img src="/2020/08/28/svd-program/1.png" alt><br>保留5位奇异值：<br><img src="/2020/08/28/svd-program/2.png" alt><br>保留10位奇异值：<br><img src="/2020/08/28/svd-program/3.png" alt><br>保留50位奇异值：<br><img src="/2020/08/28/svd-program/4.png" alt><br>保留100位奇异值：<br><img src="/2020/08/28/svd-program/5.png" alt></p>
<p><strong>保留的奇异值位数越多，保留的信息就越多</strong></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>实例</tag>
      </tags>
  </entry>
  <entry>
    <title>通俗理解奇异值分解</title>
    <url>/2020/08/27/svd-pca/</url>
    <content><![CDATA[<p>参考链接：</p>
<ol>
<li>[<a href="https://zhuanlan.zhihu.com/p/29846048">https://zhuanlan.zhihu.com/p/29846048</a>]</li>
<li>[<a href="https://www.zhihu.com/question/41120789/answer/481966094">https://www.zhihu.com/question/41120789/answer/481966094</a>]</li>
</ol>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>&emsp; &emsp;机器算法的核心就是如何妥善的处理数据，但是当我们接触到一大堆原始杂乱的陌生数据时，往往会感到手足无措，因此我们需要对原始数据压缩，从而找到影响结果变化的主要因素与次要因素，奇异值分解就为我们提供了相应的方法，便于我们从茫茫的数据中找到关键因素，当然这种方法不仅仅局限于数据压缩，还有其他强大的功能，在此就不一 一介绍了。</p>
<a id="more"></a>
<h2 id="特征值分解"><a href="#特征值分解" class="headerlink" title="特征值分解"></a>特征值分解</h2><p>&emsp; &emsp;在正式接触奇异值分解之前，我们先来回顾在线性代数中学到的特征值分解(对角化)。首先特征值分解具有一定的局限性，只能应用于方阵，并且该方阵还要满足每个特征值对应的特征向量线性无关的最大个数等于该特征值的重数才能够对角化。对于n维可对角化的矩阵A可写成以下形式：</p>
<script type="math/tex; mode=display">A=V \Lambda V^{-1}</script><p>具体表现形式为：</p>
<script type="math/tex; mode=display">A=\left[\begin{array}{}
\mid & \mid & \mid \\
\mathbf{v}_{1} & \mathbf{v}_{2} & \mathbf{v}_{3} \\
\mid & \left.\right|& \mid
\end{array}\right]\left[\begin{array}{}
\left.\mid \begin{array}{}
\lambda_{1} & 0 & 0 \\
0 & \lambda_{2} & 0 \\
0 & 0 & \lambda_{3}
\end{array}\right]\left[\left.\begin{array}{}
\mid & \mid & \mid \\
\mathbf{v}_{1} & \mathbf{v}_{2} & \mathbf{v}_{3}\\
\mid & \left.\right|& \mid
\end{array}\right]^{-1}\right.
\end{array}\right.</script><p>上式中，v1,v2,v3为矩阵A的特征向量，一般情况下我们会把特征向量标准化，即满足|| V ||=1。然而，这种特征值分解只能应用于部分方阵。然而，在我们实际的数据中，大部分都并非方阵，因此我们需要对这种方法进行拓展，从而提出了奇异值分解，来应对普遍情况。</p>
<h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p>&emsp; &emsp;$ A A^{T} $与$ A^{T} A $这两个矩阵对于奇异值分解来说是十分重要的两种矩阵，它们具有以下特征：<br><br>&emsp; &emsp;&emsp; &emsp;1.对于任意维度的矩阵A，都存在$ A A^{T} $与$ A^{T} A $；<br><br>&emsp; &emsp;&emsp; &emsp;2.实对称矩阵，特征向量正交；<br>&emsp; &emsp;&emsp; &emsp;3.方阵；<br>&emsp; &emsp;&emsp; &emsp;4.半正定；<br>&emsp; &emsp;&emsp; &emsp;5.两者具有相同的正数特征值，该特征值得算术平方根称之为奇异值<br>&emsp; &emsp;&emsp; &emsp;6.两者的秩相等，且与矩阵A相同。<br>&emsp; &emsp;这样的话，我们令$ u_{i} $为矩阵$ A A^{T} $的特征向量，令$ v_{i} $为矩阵$ A^{T} A $的特征向量,则我们称由$ u_{i} $，$ v_{i} $构成的矩阵$U$，$V$为矩阵A的奇异向量，具体表达式如下：<br><img src="/2020/08/27/svd-pca/1.png" alt><br>也就是说：</p>
<script type="math/tex; mode=display">\left(A^{T} A\right) v_{i}=\lambda_{i} v_{i}</script><script type="math/tex; mode=display">\left(A A^{T}\right) u_{i}=\lambda_{i} u_{i}</script><p>&emsp; &emsp;根据之前$ A A^{T} $与$ A^{T} A $的性质可知，我们可以通过正交分解使得$U,V$为正交矩阵。有了上述的基础概念，下面引入核心公式，奇异值分解认为，任意维度的矩阵A都可以被分解为以下形式：</p>
<script type="math/tex; mode=display">A=U S V^{T}</script><p>其中，$U,V$就是矩阵A的奇异向量；S是对角阵，由奇异值构成。矩阵的排列顺序，一般按照奇异值从大到小排列，方便统一化管理和后续操作：<br><img src="/2020/08/27/svd-pca/2.png" alt><br>那么奇异值是如何计算的呢？</p>
<script type="math/tex; mode=display">A=U \Sigma V^{T} \Rightarrow A^{T}=V \Sigma U^{T} \Rightarrow A^{T} A=V \Sigma U^{T} U \Sigma V^{T}=V \Sigma^{2} V^{T}</script><p>所以说奇异值矩阵等于特征值矩阵的算术平方根。相比于之前的特征值分解，SVD可以应用于任意维度的矩阵，并且U,V是正交矩阵，S是对角阵，具体应用的时候会非常的方便。</p>
<h2 id="奇异值分解矩阵"><a href="#奇异值分解矩阵" class="headerlink" title="奇异值分解矩阵"></a>奇异值分解矩阵</h2><p>为了让大家有一个深刻的印象，我举一个简单的例子来具体说明SVD是如何分解矩阵的。<br>对于矩阵A：</p>
<script type="math/tex; mode=display">A=\left(\begin{array}{ccc}
3 & 2 & 2 \\
2 & 3 & -2
\end{array}\right)</script><p>计算$ A A^{T} $与$ A^{T} A $的值得：</p>
<script type="math/tex; mode=display">A A^{T}=\left(\begin{array}{cc}
17 & 8 \\
8 & 17
\end{array}\right), \quad A^{T} A=\left(\begin{array}{ccc}
13 & 12 & 2 \\
12 & 13 & -2 \\
2 & -2 & 8
\end{array}\right)</script><p>上述矩阵都是半正定的，其特征值为25、9，也就是说A的奇异值为5、2。另外，这两个矩阵之间的联系为：<br><img src="/2020/08/27/svd-pca/3.png" alt><br>因此SVD的表达式为：</p>
<script type="math/tex; mode=display">A=U S V^{T}=\left(\begin{array}{cc}
1 / \sqrt{2} & 1 / \sqrt{2} \\
1 / \sqrt{2} & -1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{ccc}
5 & 0 & 0 \\
0 & 3 & 0
\end{array}\right)\left(\begin{array}{rrr}
1 / \sqrt{2} & 1 / \sqrt{2} & 0 \\
1 / \sqrt{18} & -1 / \sqrt{18} & 4 / \sqrt{18} \\
2 / 3 & -2 / 3 & -1 / 3
\end{array}\right)</script><p>上述所说的只是基本应用，下面我对SVD的公式进行基本的变换：</p>
<script type="math/tex; mode=display">\begin{array}{l}
A=U S V^{T} \\
A V=U S
\end{array}</script><p>也就是说SVD认为矩阵A是其所有奇异值与其对应的奇异向量的组合：</p>
<script type="math/tex; mode=display">A=\sigma_{1} u_{1} v_{1}^{T}+\ldots+\sigma_{r} u_{r} v_{r}^{T}</script><p>上述公式是理解SVD矩阵分解的关键，它提供了一种重要的方法去拆解m*n数据矩阵至r个元素，现在我们在来重新分析下之前的例子是如何工作的：</p>
<script type="math/tex; mode=display">A=\left(\begin{array}{ccc}
3 & 2 & 2 \\
2 & 3 & -2
\end{array}\right)</script><p>矩阵A可以被拆解为：</p>
<script type="math/tex; mode=display">\begin{aligned}
&=5\left(\begin{array}{ccc}
1 / \sqrt{2} \\
1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{cccc}
1 / \sqrt{2} & 1 / \sqrt{2} & 0
\end{array}\right)+3\left(\begin{array}{cc}
1 / \sqrt{2} \\
-1 / \sqrt{2}
\end{array}\right)\left(\begin{array}{ccc}
1 / \sqrt{18} & -1 / \sqrt{18} & 4 / \sqrt{18}
\end{array}\right)\\
&=\left(\begin{array}{ccc}
3 & 2 & 2 \\
2 & 3 & -2
\end{array}\right)
\end{aligned}</script><h2 id="奇异值分解应用"><a href="#奇异值分解应用" class="headerlink" title="奇异值分解应用"></a>奇异值分解应用</h2><h3 id="moore-penrose伪逆"><a href="#moore-penrose伪逆" class="headerlink" title="moore-penrose伪逆"></a>moore-penrose伪逆</h3><p>对于线性方程组而言，我们往往需要计算矩阵的逆，如下图所示：</p>
<script type="math/tex; mode=display">\begin{aligned}
A x &=b \\
x &=A^{-1} b
\end{aligned}</script><p>但是对于大部分矩阵而言，都没有办法求其逆矩阵，因此在求解方程组时需要逆矩阵的替代品，moore-penrose伪逆提供了这样的一种方法，可以计算矩阵A的伪逆值，使得$ \left|A A^{+}-I_{n}\right|_{2} $ 最小，也就是伪逆矩阵$A^{+}$的相关性质最接近其逆矩阵，因此线性方程组的解就可以被估计为：</p>
<script type="math/tex; mode=display">\begin{aligned}
A x &=b \\
x & \approx A^{+} b
\end{aligned}</script><p>其中，为逆矩阵$A^{+}$的具体求解方式如下：</p>
<script type="math/tex; mode=display">\begin{array}{l}
A x=b \\
(U, D, V) \leftarrow \operatorname{svd}(A) \\
A^{+}=V D^{+} U^{T} \\
x=A^{+} b
\end{array}</script><p>我们以一个简单的例子来具体说明：<br>$A=\left[\begin{array}{ll}1 &amp; 1 \\ 1 &amp; 1\end{array}\right] \quad$ </p>
<p>$A=U \Sigma V^{T}=\frac{1}{\sqrt{2}}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; -1\end{array}\right]\left[\begin{array}{cc}2 &amp; 0 \\ 0 &amp; 0\end{array}\right] \frac{1}{\sqrt{2}}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; -1\end{array}\right]$</p>
<p>$A^{+}=V \Sigma^{+} U^{T}=\frac{1}{\sqrt{2}}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; -1\end{array}\right]\left[\begin{array}{cc}1 / 2 &amp; 0 \\ 0 &amp; 0\end{array}\right] \frac{1}{\sqrt{2}}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; -1\end{array}\right]=\frac{1}{4}\left[\begin{array}{cc}1 &amp; 1 \\ 1 &amp; 1\end{array}\right]$</p>
<p>&emsp;&emsp;moore-penrose伪逆的求解是基于奇异值分解的，通过上面的例子我们可以看出即便方阵不可逆，我们也能通过奇异值分解的方法得到其伪逆矩阵，也就是说该方法对于任意维度的任意矩阵都能适用。此外，待求矩阵A 的列数如果大于行数，得到就是所有可行解中L2范数最小的一个，如果相反，就是L2(Ax−y)最小</p>
<h3 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h3><p>&emsp;&emsp;在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵:</p>
<script type="math/tex; mode=display">A_{m \times n}=U_{m \times m} \Sigma_{m \times n} V_{n \times n}^{T} \approx U_{m \times k} \Sigma_{k \times k} V_{k \times n}^{T}</script><p>其中k要比n小很多，也就是一个大的矩阵A可以用三个小的矩阵$U_{m \times k}, \sum_{k \times k}, V_{k \times n}^{T}$来表示,这样就可以以舍弃部分精度来减少数据量。如下图所示，现在我们的矩阵A只需要灰色的部分的三个小矩阵就可以近似描述了。<br><img src="/2020/08/27/svd-pca/4.png" alt><br>由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪，PCA具体原理可以参考<a href="https://www.zhihu.com/question/41120789/answer/481966094"><strong>如何通俗易懂地讲解什么是 PCA 主成分分析</strong></a>。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title>深度理解多层感知机（MLP）</title>
    <url>/2020/08/30/bp/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>&emsp;&emsp;本篇blog将介绍神经网络的入门基础——深度感知机，简单介绍下深度感知机的结构，分析数据是如何前馈的，误差又是如何后馈，即误差如何反向传播，这是神经网络优化算法的核心。</p>
<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>&emsp;&emsp;神经网络的灵感取自于生物上的神经元细胞，如下图所示：<br><img src="/2020/08/30/bp/1.PNG" alt><br>&emsp;&emsp;这是人体神经元的基本构成，其中树突主要用于接收其他神经元的信号，轴突用于输出该神经元的信号，数以万计的神经元相互合作，使得我们人类能够进行高级的思考，能够不断地对新事物进行学习。因此，我们就希望仿照人类神经网络的结构，搭建一种人为的神经网络结构，从而使其能够完成一些计算任务，这也是神经网络名字的由来。</p>
<a id="more"></a>
<h2 id="节点结构"><a href="#节点结构" class="headerlink" title="节点结构"></a>节点结构</h2><p>&emsp;&emsp;神经网络中计算的基本单元是神经元，一般称作「节点」（node）或者「单元」（unit）。每个节点可以从其他节点接收输入，或者从外部源接收输入，然后计算输出。每个输入都各自的「权重」（weight，即 w），用于调节该输入对输出影响的大小，节点的结构如图所示：<br><img src="/2020/08/30/bp/3.png" alt><br>&emsp;&emsp;其中x1,x2作为该节点的输入，其权重分别为 w1 和 w2。同时，还有配有「偏置b」（bias）的输入 ，偏置的主要功能是为每一个节点提供可训练的常量值（在节点接收的正常输入以外）。神经元中偏置的作用，详见这个链接：(<a href="https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks">https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks</a>)<br>&emsp;&emsp;神经元的输出 Y 按照上图中的计算方式进行，其中函数 f 叫做激活函数，一般采用非线性函数，其作用是将非线性引入神经元的输出。因为大多数现实世界的数据都是非线性的，我们希望神经元能够学习非线性的函数表示，如果激活函数仍然是线性的，那么根据线性可叠加的原理，整个函数模型仍然是线性的，那么就无法解决我们遇到的问题（例如异或），因此激活函数非线性这一点非常重要。<br>&emsp;&emsp;每个（非线性）激活函数都接收一个数字，并进行特定、固定的数学计算 。在实践中，可能会碰到几种激活函数：</p>
<ol>
<li><p>Sigmoid（S 型激活函数）：输入一个实值，输出一个 0 至 1 间的值 σ(x) = 1 / (1 + exp(−x))</p>
</li>
<li><p>tanh（双曲正切函数）：输入一个实值，输出一个 [-1,1] 间的值 tanh(x) = 2σ(2x) − 1</p>
</li>
<li><p>ReLU：ReLU 代表修正线性单元。输出一个实值，并设定 0 的阈值（函数会将负值变为零）f(x) = max(0, x)<br><img src="/2020/08/30/bp/4.png" alt><br>如图所示，该图为不同的激活函数的曲线图，第一种就是逻辑回归使用的激活函数，往往应用于二分类；在神经网络中，我们往往更倾向于使用最后一种relu函数作为激活函数。</p>
</li>
</ol>
<h2 id="MLP结构"><a href="#MLP结构" class="headerlink" title="MLP结构"></a>MLP结构</h2><p><img src="/2020/08/30/bp/5.png" alt><br>MLP的结构如图所示，主要由三部分组成，至少要有一个隐藏层。</p>
<ol>
<li><p>输入节点（Input Nodes）：输入节点从外部世界提供信息，总称为「输入层」。在输入节点中，不进行任何的计算，仅向隐藏节点传递信息。</p>
</li>
<li><p>隐藏节点（Hidden Nodes）：隐藏节点和外部世界没有直接联系（由此得名）。这些节点进行计算，并将信息从输入节点传递到输出节点。隐藏节点总称为「隐藏层」。尽管一个前馈神经网络只有一个输入层和一个输出层，但网络里可以没有隐藏层（如果没有隐藏层，激活函数选择sigmod，那么就变成逻辑回归了），也可以有多个隐藏层。</p>
</li>
<li><p>输出节点（Output Nodes）：输出节点总称为「输出层」，负责计算，并从网络向外部世界传递信息。</p>
</li>
</ol>
<h2 id="数据前馈"><a href="#数据前馈" class="headerlink" title="数据前馈"></a>数据前馈</h2><p><img src="/2020/08/30/bp/6.png" alt><br>以之前的结构为例：</p>
<p>(1) 输入层：输入层有三个节点。偏置节点值为 1。其他两个节点，X1 和 X2 取自外部输入（皆为根据输入数据集取的数字值）。和上文讨论的一样，在输入层不进行任何计算，所以输入层节点的输出是 1、X1 和 X2 三个值被传入隐藏层。</p>
<p>(2) 隐藏层：隐藏层也有三个节点，偏置节点输出为 1。隐藏层其他两个节点的输出取决于输入层的输出（1，X1，X2）以及连接上所附的权重。图中显示了隐藏层（高亮）中一个输出的计算方式。其他隐藏节点的输出计算同理。需留意 <em>f </em>指代激活函数。这些输出会作为输出层的输入。</p>
<p>(3) 输出层：输出层有两个节点，从隐藏层接收输入，并执行类似高亮出的隐藏层的计算。这些作为计算结果的计算值（Y1 和 Y2）就是多层感知器的输出。</p>
<p>&emsp;&emsp;给出一系列特征 X = (x1, x2, …) 和目标 Y，一个多层感知器可以以分类或者回归为目的，学习到特征和目标之间的关系,从而建立模型。为了更好的理解多层感知器，我们举一个例子。假设我们有这样一个学生分数数据集：<br><img src="/2020/08/30/bp/7.png" alt><br>&emsp;&emsp;两个输入栏表示了学生学习的时间和期中考试的分数。最终结果栏可以有两种值，1 或者 0，来表示学生是否通过的期末考试。例如，我们可以看到，如果学生学习了 35 个小时并在期中获得了 67 分，他 / 她就会通过期末考试。现在我们假设我们想预测一个学习了 25 个小时并在期中考试中获得 70 分的学生是否能够通过期末考试。<br><img src="/2020/08/30/bp/8.png" alt></p>
<p>&emsp;&emsp;这是一个二元分类问题，那么多层感知机又是如何利用这么多的数据的到我们想要的结果呢？<br>&emsp;&emsp;首先，对网络中所有的权重进行随机分配，假设从输入连接到这些节点的权重分别为 w1、w2 和 w3（如图所示）。神经网络会将第一个训练样本作为输入，那么网络的输入=[35, 67]，涉及到的节点的输出 V 可以按如下方式计算（<em>f</em> 是类似 Sigmoid 的激活函数）：V = f(1×w1 + 35×w2 + 67×w3），最后的得到我们的输出结果，假设输出层两个节点的输出概率分别为 0.4 和 0.6（因为权重随机，输出也会随机），但是我们期望的网络输出（目标）=[1, 0]，可以看出两者之间的差距还是蛮大的，那么我们该如何调整权重，使得输出的结果更加接近目标呢？这一点，我将在下一部分讲解，也就是我们的优化算法。</p>
<h2 id="误差后馈"><a href="#误差后馈" class="headerlink" title="误差后馈"></a>误差后馈</h2><p>下面我们将介绍一个多层感知器如何调整权重：</p>
<blockquote>
<blockquote>
<p>&emsp;&emsp;反向传播误差，通常缩写为「BackProp」，是几种训练人工神经网络的方法之一。这是一种监督学习方法，即通过标记的训练数据来学习（有监督者来引导学习）。简单说来，BackProp 就像「从错误中学习」。监督者在人工神经网络犯错误时进行纠正。一个人工神经网络包含多层的节点: 输入层，隐藏层和输出层。相邻层节点的连接都有配有「权重」。学习的目的是为这些边缘分配正确的权重。通过输入向量，这些权重可以决定输出向量。在监督学习中，训练集是已标注的。这意味着对于一些给定的输入，我们知道期望的输出（标注）。<br>&emsp;&emsp;反向传播算法：最初，所有的边权重（edge weight）都是随机分配的。对于所有训练数据集中的输入，人工神经网络都被激活，并且观察其输出。这些输出会和我们已知的、期望的输出进行比较，误差会「传播」回上一层。该误差会被标注，权重也会被相应的「调整」。该流程重复，直到输出误差低于制定的标准。上述算法结束后，我们就得到了一个学习过的人工神经网络，该网络被认为是可以接受「新」输入的。该人工神经网络可以说从几个样本（标注数据）和其错误（误差传播）中得到了学习。</p>
</blockquote>
</blockquote>
<p>&emsp;&emsp;现在我们知道了反向传播的原理，我们回到上面的学生分数数据集。<br><img src="/2020/08/30/bp/9.jpg" alt><br>&emsp;&emsp;如图所示的多层感知器，其输入层有两个节点（除了偏置节点以外），两个节点分别接收「学习小时数」和「期中考试分数」。感知器也有一个包含两个节点的隐藏层（除了偏置节点以外）。输出层也有两个节点——上面一个节点输出「通过」的概率，下面一个节点输出「不通过」的概率。<br>&emsp;&emsp;在分类任务中，我们通常在感知器的输出层中使用 Softmax 函数作为激活函数，以保证输出的是概率并且相加等于 1。Softmax 函数接收一个随机实值的分数向量，转化成多个介于 0 和 1 之间、并且总和为 1 的多个向量值。所以，在这个例子中：概率（Pass）+概率（Fail）=1<br>&emsp;&emsp;在上一小节，我主要介绍了数据的前向传播，那么下面来重点讲解误差反向传播和权重更新：计算输出节点的总误差，并将这些误差用反向传播算法传播回网络，以计算梯度。接下来，我们使用类似梯度下降之类的算法来调整网络中的所有权重，目的是减少输出层的误差。下图展示了这一过程（暂时忽略图中的数学等式）。<br><img src="/2020/08/30/bp/10.jpg" alt><br>&emsp;&emsp;假设附给节点在反向传播和权重调整之后的新权重分别是 w4，w5 和 w6。接着用我们数据集中的其他训练样本来重复这一过程，不断迭代，这样，我们的网络就可以被视为学习了这些例子，从而使得输出误差不断减小，直到误差减小到一个合适的范围，那么我们就可以停止迭代了。这样我们就能够获得较为精准的预测模型。现在，如果我们想预测一个学习了 25 个小时、期中考试 70 分的学生是否能通过期末考试，我们可以通过前向传播步骤来计算 Pass 和 Fail 的输出概率。<br>更具体的bp算法推导可以参考：<a href="http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html">http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html</a></p>
<h2 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h2><p>&emsp;&emsp;过拟合是指训练误差小，但是泛化误差较大；欠拟合是指训练误差大，泛化误差也很大。下面，我主要针对神经网络做一些相关的讨论<br>&emsp;&emsp;神经网络相比于其他的机器学习算法，其功能更为强大，只要给予神经网络足够多的节点，理论上它就能拟合任意的模型，但是也有缺点，就是容易产生过拟合。那么产生过拟合的根本原因是什么呢？那是因为我们并不能保证训练样本完全正确，也就是可能存在部分的样本存在误导性，所以当神经网络功能过于强大时，很容易连训练样本中的错误样本也学习了，从而导致只能具有很低训练误差，而不能拥有较低的泛化误差，也就是我们所说的过拟合。最常见的方法之一是提前终止学习，如图所示：<br><img src="/2020/08/30/bp/11.png" alt><br>&emsp;&emsp;在合适的迭代次数终止算法从而达到最优容量（泛化误差最低），但是最佳迭代次数并不好找，往往需要多次测试才有可能找到，因此更常用的方法是“Dropout”防止过拟合，下面来重点讲解：</p>
<h3 id="什么是Dropout"><a href="#什么是Dropout" class="headerlink" title="什么是Dropout"></a>什么是Dropout</h3><p>&emsp;&emsp;在2012年，Hinton在其论文《Improving neural networks by preventing co-adaptation of feature detectors》中提出Dropout。当一个复杂的前馈神经网络被训练在小的数据集时，容易造成过拟合。为了防止过拟合，可以通过阻止特征检测器的共同作用来提高神经网络的性能。<br>&emsp;&emsp;在2012年，Alex、Hinton在其论文《ImageNet Classification with Deep Convolutional Neural Networks》中用到了Dropout算法，用于防止过拟合。并且，这篇论文提到的AlexNet网络模型引爆了神经网络应用热潮，并赢得了2012年图像识别大赛冠军，使得CNN成为图像分类上的核心算法模型。<br>&emsp;&emsp;随后，又有一些关于Dropout的文章《Dropout:A Simple Way to Prevent Neural Networks from Overfitting》、《Improving Neural Networks with Dropout》、《Dropout as data augmentation》。<br>&emsp;&emsp;从上面的论文中，我们能感受到Dropout在深度学习中的重要性。那么，到底什么是Dropout呢？<br>&emsp;&emsp;Dropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。<br>&emsp;&emsp;Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图1所示。<br><img src="/2020/08/30/bp/12.png" alt></p>
<h3 id="Dropout工作流程及使用"><a href="#Dropout工作流程及使用" class="headerlink" title="Dropout工作流程及使用"></a>Dropout工作流程及使用</h3><h4 id="Dropout具体工作流程"><a href="#Dropout具体工作流程" class="headerlink" title="Dropout具体工作流程"></a>Dropout具体工作流程</h4><p>假设我们要训练这样一个神经网络，如图所示。<br><img src="/2020/08/30/bp/13.png" alt><br>输入是x，输出是y，正常的流程是：我们首先把x通过网络前向传播，然后把误差反向传播以决定如何更新参数让网络进行学习。使用Dropout之后，过程变成如下：</p>
<p>（1）首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图中虚线为部分临时被删除的神经元）<br><img src="/2020/08/30/bp/14.png" alt><br>（2） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。</p>
<p>（3）然后继续重复这一过程：</p>
<ul>
<li>恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）</li>
<li>从隐藏层神经元中随机选择一个一半大小（这个量可以自己选）的子集临时删除掉（备份被删除神经元的参数）。</li>
<li>对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。</li>
</ul>
<h3 id="为什么说Dropout可以解决过拟合"><a href="#为什么说Dropout可以解决过拟合" class="headerlink" title="为什么说Dropout可以解决过拟合"></a>为什么说Dropout可以解决过拟合</h3><p><strong>（1）取平均的作用</strong>： 先回到标准的模型，即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</p>
<p><strong>（2）减少神经元之间复杂的共适应关系</strong>： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。</p>
<p><strong>（3）Dropout类似于性别在生物进化中的角色</strong>：物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。</p>
<h2 id="初始参数的选取"><a href="#初始参数的选取" class="headerlink" title="初始参数的选取"></a>初始参数的选取</h2><p>在许多参考文献中都提到，对于神经网络而言。模型与优化的选取至关重要，当然这无可厚非，但是初始值的选取对于模型的训练也有一定程度的影响，不合适的初始值可能会耗费大量的训练时间，甚至有可能无法达到我们预期的目标，因此在这里简单介绍下初始值选取的一般规则：<br>(1) <strong>禁止将所有的权重均设置为同一个常数c，否则bp算法将会失效，所有的权重将不会更新！</strong><br>(2) 一般令权重W的初始值服从0均值的高斯分布，该分布的方差应该选择合适的大小，如果过小会导致梯度消失，过大会导致权重无法收敛。<br>(3) 方差选取的参考意见：第一种：$\sigma^{2}=2 /\left(n_{i n}+n_{o u t}\right)$，其中$n_{\text {in}}$ 与 $n_{\text {out}}$分别为该层的上一层节点数量与下一层节点数量；第二种：$\sigma^{2}=2 / n_{\text {in }}$ </p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>入门</tag>
        <tag>课程</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>
